---
title: "STAT 501 Project"
output:
  html_notebook: default
  pdf_document: default
---
#  Multiple Linear  Regression Model to Predict Electricity Demand of Five Cities in Spain
## Links documentation

* [Github Code](https://github.com/ShaileshWasti/STAT-501-Project.git)
* [Overleaf Report](https://www.overleaf.com/9698321993jvwcjbdtymzk)
* [Kaggle Data](https://www.kaggle.com/datasets/nicholasjhana/energy-consumption-generation-prices-and-weather?select=weather_features.csv)

Note: If you need permission to use some links, please email at: *sjw6236@psu.edu* 

## Install all Packages and Import Library 

### Packages installation
```{r}
# install.packages("dplyr")
# install.packages('car')
# install.packages("ggplot2")

```

### Import library
```{r,message=FALSE, warning=FALSE, results='hide'}
#load package here
library(tidyverse)
library(gridBase)
library(grid)
library(dplyr)
library(car)
library (GGally)
library(readxl) 
library(ggplot2)
library(broom)
library(ggpubr)
library(corrplot)
library(MASS)
library(Stat2Data)
library(olsrr)
library(leaps)
library(leaps)
library(caret)
library(lmtest)
library(nortest)
library(Metrics)
library(broom.mixed)
library(ggbreak)
library(gridExtra)
library(tinytex)
library(randomForest) 
library(ranger)
options(scipen=999)  # turn-off scientific notation like 1e+48
theme_set(theme_bw())  # pre-set the bw theme.

```

## Data Processing
### Load weather data
```{r}
## Read raw data: weather features
weather_raw<-read.csv(file = "Data/weather_features.csv")
head(weather_raw,4) 
```


```{r}
# All the columns in the raw data
weather_raw_fields <- colnames(weather_raw) 
print(weather_raw_fields)
```

### Remove all duplicate data from the weather data 

```{r}
# All the duplicate data based on date 
weather_duplicate<-weather_raw[duplicated(weather_raw[,1:2]),]
weather <- weather_raw[!duplicated(weather_raw[,1:2]), ]
```

### Load energy data set modified from Kagel
#### Calculation: 
*(Keith,Can you explain here in bullets how new excel sheet is generated in bullet?)*
* Use $$ for math equation, if any and reference the original data, and other links like I did in the beginning of this code


```{r}
energy_raw<-read_excel("Data/energy_dataset-KO.xlsx")
```


* Five cities are:
  + Valencia
  + Barcelona
  + Bilbao
  + Seville
  + Madrid


### Merge Energy Demand and Weather to one data set based on cities
```{r}
# Valencia
Weather_Valencia <- weather[weather$city_name == 'Valencia',]
Data_Valencia <-merge(Weather_Valencia, energy_raw[,c("time","Valencia")], by.x = "dt_iso", by.y = "time")
colnames(Data_Valencia)[colnames(Data_Valencia) == "Valencia"] <- "energy"

# Barcelona
Weather_Barcelona <- weather[weather$city_name == " Barcelona",]
Data_Barcelona <-merge(Weather_Barcelona, energy_raw[,c("time","Barcelona")], by.x = "dt_iso", by.y = "time")
colnames(Data_Barcelona)[colnames(Data_Barcelona) == "Barcelona"] <- "energy"

# Bilbao 
Weather_Bilbao <- weather[weather$city_name == 'Bilbao',]
Data_Bilbao <-merge(Weather_Bilbao, energy_raw[,c("time","Bilboa")], by.x = "dt_iso", by.y = "time")
colnames(Data_Bilbao)[colnames(Data_Bilbao) == "Bilboa"] <- "energy"
  
# Seville
Weather_Seville <- weather[weather$city_name == 'Seville',]
Data_Seville <-merge(Weather_Seville, energy_raw[,c("time","Seville")], by.x = "dt_iso", by.y = "time")
colnames(Data_Seville)[colnames(Data_Seville) == "Seville"] <- "energy"

# Madrid
Weather_Madrid <- weather[weather$city_name == 'Madrid',]
Data_Madrid <-merge(Weather_Madrid, energy_raw[,c("time","Madrid")], by.x = "dt_iso", by.y = "time")
colnames(Data_Madrid)[colnames(Data_Madrid) == "Madrid"] <- "energy"
```

### Write all the data in separate cityname.csv sheets

```{r}
write.csv(Data_Barcelona,"Data/Barcelona.csv", row.names = TRUE)
write.csv(Data_Valencia,"Data/Valencia.csv", row.names = TRUE)
write.csv(Data_Bilbao,"Data/Bilbao.csv", row.names = TRUE)
write.csv(Data_Seville,"Data/Seville.csv", row.names = TRUE)
write.csv(Data_Madrid,"Data/Madrid.csv", row.names = TRUE)

```




## BARCELONA DATA
```{r}
# Data with all predictors and response variable 
## Read raw data: Barcelona
Barcelona_raw<-read.csv(file = "Data/Barcelona.csv")

# Rename column names 
colnames(Barcelona_raw)[colnames(Barcelona_raw) == "X"] <- "DataID"
colnames(Barcelona_raw)[colnames(Barcelona_raw) == "dt_iso"] <- "time"

# Select only the columns of interest

Barcelona_Data<-Barcelona_raw %>% 
  dplyr::select(DataID,time,temp,humidity,pressure,wind_speed,rain_1h,rain_3h,snow_3h,weather_main,energy)
```

### See all the weather description in weather_main
```{r}
print(unique(Barcelona_Data$weather_main))
```

### Give integer values for each description

```{r}
Barcelona_Data<-transform(Barcelona_Data, weather_main = factor(weather_main, 
      levels = c("clear", "clouds", "drizzle","dust","fog","haze","mist","rain","smoke","snow","squall","thunderstorm"),
       labels = c(1:12)))

Weather_description <- data.frame (Weather_Description  = c("clear", "clouds", "drizzle",
              "dust","fog","haze","mist","rain","smoke","snow","squall","thunderstorm"),
                  Index = c(1:12)
                  )
print(Weather_description)
```


### More Data Processing


```{r}
# Add two columns of rain duration
Barcelona_Data["rain_duration"] <- Barcelona_Data$rain_1h + Barcelona_Data$rain_3h 

# Rename Column Names for clarity 
Barcelona_Data <-merge(Barcelona_Data, energy_raw[,c("time","Specified Categorical Variable:\r\nTime Band (Peak, Off-Peak, Mid-Peak)")], by.x = "time", by.y = "time")
colnames(Barcelona_Data )[colnames(Barcelona_Data ) == "Specified Categorical Variable:\r\nTime Band (Peak, Off-Peak, Mid-Peak)"] <- "time_band"

Barcelona_Data <-merge(Barcelona_Data, energy_raw[,c("time","Specified Categorical Variable:\r\nSeason (Spring, Summer, Autumn, Winter)")], by.x = "time", by.y = "time")
colnames(Barcelona_Data )[colnames(Barcelona_Data ) == "Specified Categorical Variable:\r\nSeason (Spring, Summer, Autumn, Winter)"] <- "season"

Barcelona_Data <-merge(Barcelona_Data, energy_raw[,c("time","Specified Categorical Variable:\r\nTime of day (Day vs Night)")], by.x = "time", by.y = "time")
colnames(Barcelona_Data )[colnames(Barcelona_Data ) == "Specified Categorical Variable:\r\nTime of day (Day vs Night)"] <- "day_night"


colnames(Barcelona_Data )[colnames(Barcelona_Data ) == "snow_3h"] <- "snow_duration"

Barcelona_Data<-subset(Barcelona_Data,select=-c(rain_1h,rain_3h,DataID))

colnames(Barcelona_Data )[colnames(Barcelona_Data ) == "time"] <- "time_ID"

Barcelona_Data <- Barcelona_Data [, c("time_ID","temp","humidity","pressure","wind_speed","rain_duration","snow_duration","day_night","time_band","season","weather_main","energy")]

colnames(Barcelona_Data )[colnames(Barcelona_Data ) == "energy"] <- "energy_demand"

```

### Removal of rows with errorenous data 

[Reference to average weather data for Barcelona](https://weatherspark.com/y/47213/Average-Weather-in-Barcelona-Spain-Year-Round)


```{r}
Barcelona_Data[Barcelona_Data$temp < 270 ,]
Barcelona_Data<- subset(Barcelona_Data, temp>270) # remove temperature less than 270K
Barcelona_Data<- subset(Barcelona_Data, pressure >900 & pressure <1050) # remove pressure outside [900,1050]mbar
Barcelona_Data<-subset(Barcelona_Data, energy_demand >10) # Remove all 0 demand from the data
```
```{r}
Barcelona.rain_duration <- ggplot(Barcelona_Data) +
          geom_point( aes(c(1:length(rain_duration)),rain_duration)) +
          labs(subtitle="Rain Duration for Barcelona", 
          y="rain duration", 
          x="# data point") 

Barcelona.humidity <- ggplot(Barcelona_Data) +
          geom_point( aes(c(1:length(humidity)),humidity)) +
          labs(subtitle="Humidity for Barcelona", 
          y="humidity", 
          x="# data point")  

Barcelona.wind_speed <- ggplot(Barcelona_Data) +
          geom_point( aes(c(1:length(wind_speed)),wind_speed)) +
          labs(subtitle="Wind Speed for Barcelona", 
          y="wind_speed", 
          x="# data point")  

Barcelona.pressure <- ggplot(Barcelona_Data) +
          geom_point( aes(c(1:length(pressure)),pressure)) +
          labs(subtitle="Pressure of Barcelona", 
          y="pressure", 
          x="# data point")  

ggarrange(Barcelona.rain_duration,Barcelona.humidity,Barcelona.wind_speed,Barcelona.pressure,
          labels = c("(A)", "(B)", "(C)","(D)"),
          ncol = 2, nrow = 2)

```

```{r}
FullModel <- lm(energy_demand ~ temp + humidity + pressure + wind_speed + rain_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = Barcelona_Data)
summary(FullModel)$adj.r.squared
```

Adjusted R2 of the model is very low *0.3127159*. 

We perform Autocorrelation and Partial Autocorrelation to incorporate temporal variability/influence in the model. 



## Generation of AutoCorrelation and Partial Autocorrelation profile 
```{r}

# Opening the graphical device
# Customizing the output
pdf("acfpcf.pdf",         # File name
    width = 14, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model 
   

par(mfrow = c(1, 2))
Barcelona.acr <- acf(Barcelona_Data$energy_demand,plot = FALSE)
plot(Barcelona.acr, main = "Energy Demand of Barcelona", res=600, cex.main = 3)
grid(10,10)
Barcelona.pacr<- pacf(Barcelona_Data$energy_demand,plot =FALSE)
plot(Barcelona.pacr, main = "Energy Demand of Barcelona",res=600, cex.main = 3)
grid(10,10)

# Closing the graphical device
dev.off() 




pacf_values <-as.data.frame(Barcelona.pacr$acf)

pacf_values_detect<-subset(pacf_values, abs(pacf_values) >0.25) 

head(pacf_values_detect)
# Check the highest partially auto correlated energy demand in order
idx<-order(abs(pacf_values_detect$V1),decreasing = TRUE)
pacf_values_detect<- pacf_values_detect[order(abs(pacf_values_detect$V1),decreasing = TRUE),]

```



# Create energy demand lag variable, one at a time

```{r}
E_1<-Barcelona_Data$energy_demand[-1] # Get all the data except in first row 
Barcelona_Data<-Barcelona_Data[-nrow(Barcelona_Data),]
Barcelona_Data$E_1 <- E_1

E_2<-E_1[-1]
Barcelona_Data<-Barcelona_Data[-nrow(Barcelona_Data),]
Barcelona_Data$E_2 <- E_2

E_25<-Barcelona_Data$energy_demand[25:nrow(Barcelona_Data)] # Get all the data except in first row
Barcelona_Data<-Barcelona_Data[1:(nrow(Barcelona_Data)-24),]

Barcelona_Data$E_25 <- E_25
```

### Check the model now
```{r}
FullModel1 <- lm(energy_demand ~ E_1 + E_2 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = Barcelona_Data)
summary(FullModel1)

```



### Generate Working and Evaluation Data

```{r}
Barcelona_Data <- Barcelona_Data [, c("time_ID", "E_1","E_2","E_25",   "temp","humidity","pressure","wind_speed","rain_duration","snow_duration","day_night","time_band","season","weather_main","energy_demand")]


set.seed(501)
working_rows <- sample(1:nrow(Barcelona_Data),0.80*nrow(Barcelona_Data))

working_Barcelona <-Barcelona_Data[working_rows,]

working_Barcelona$index <- as.numeric(row.names(working_Barcelona))
working_Barcelona<- working_Barcelona[order(working_Barcelona$index), ]

working_Barcelona<-working_Barcelona %>%
                    relocate(index) 


evaluation_Barcelona <-Barcelona_Data[-working_rows,]
evaluation_Barcelona$index <- as.numeric(row.names(evaluation_Barcelona))
evaluation_Barcelona<- evaluation_Barcelona[order(evaluation_Barcelona$index), ]

evaluation_Barcelona<-evaluation_Barcelona %>%
                    relocate(index) 

write.csv(working_Barcelona,"Data/working_Barcelona.csv", row.names = FALSE)
write.csv(evaluation_Barcelona,"Data/evaluation_Barcelona.csv", row.names = FALSE)
```




## Valencia DATA Preparation 
```{r}

Valencia_raw<-read.csv(file = "Data/Valencia.csv")
colnames(Valencia_raw)[colnames(Valencia_raw) == "X"] <- "DataID"
colnames(Valencia_raw)[colnames(Valencia_raw) == "dt_iso"] <- "time"
Valencia_Data<-Valencia_raw %>%
  dplyr::select(DataID,time,temp,humidity,pressure,wind_speed,rain_1h,rain_3h,snow_3h,weather_main,energy)

Valencia_Data<-transform(Valencia_Data, weather_main = factor(weather_main, 
                                                              levels = c("clear", "clouds", "drizzle","dust","fog","haze","mist","rain","smoke","snow","squall","thunderstorm"),
                                                              labels = c(1:12)))

# Add two columns of rain duration
Valencia_Data["rain_duration"] <- Valencia_Data$rain_1h + Valencia_Data$rain_3h 

# Rename Column Names for clarity 
Valencia_Data <-merge(Valencia_Data, energy_raw[,c("time","Specified Categorical Variable:\r\nTime Band (Peak, Off-Peak, Mid-Peak)")], by.x = "time", by.y = "time")
colnames(Valencia_Data )[colnames(Valencia_Data ) == "Specified Categorical Variable:\r\nTime Band (Peak, Off-Peak, Mid-Peak)"] <- "time_band"

Valencia_Data <-merge(Valencia_Data, energy_raw[,c("time","Specified Categorical Variable:\r\nSeason (Spring, Summer, Autumn, Winter)")], by.x = "time", by.y = "time")
colnames(Valencia_Data )[colnames(Valencia_Data ) == "Specified Categorical Variable:\r\nSeason (Spring, Summer, Autumn, Winter)"] <- "season"

Valencia_Data <-merge(Valencia_Data, energy_raw[,c("time","Specified Categorical Variable:\r\nTime of day (Day vs Night)")], by.x = "time", by.y = "time")
colnames(Valencia_Data )[colnames(Valencia_Data ) == "Specified Categorical Variable:\r\nTime of day (Day vs Night)"] <- "day_night"


colnames(Valencia_Data )[colnames(Valencia_Data ) == "snow_3h"] <- "snow_duration"

Valencia_Data<-subset(Valencia_Data,select=-c(rain_1h,rain_3h,DataID))

colnames(Valencia_Data )[colnames(Valencia_Data ) == "time"] <- "time_ID"

Valencia_Data <- Valencia_Data [, c("time_ID","temp","humidity","pressure","wind_speed","rain_duration","snow_duration","day_night","time_band","season","weather_main","energy")]

colnames(Valencia_Data )[colnames(Valencia_Data ) == "energy"] <- "energy_demand"

VD_<- Valencia_Data[Valencia_Data$temp < 270 ,]
Valencia_Data<- subset(Valencia_Data, temp>270) # remove temperature less than 270K
Valencia_Data<- subset(Valencia_Data, pressure >900 & pressure <1050) # remove pressure outside [900,1050]mbar
Valencia_Data<-subset(Valencia_Data, energy_demand >10) # Remove all 0 demand from the data

Valencia.rain_duration <- ggplot(Valencia_Data) +
  geom_point( aes(c(1:length(rain_duration)),rain_duration)) +
  labs(subtitle="Rain Duration for Valencia", 
       y="rain duration", 
       x="# data point") 

Valencia.humidity <- ggplot(Valencia_Data) +
  geom_point( aes(c(1:length(humidity)),humidity)) +
  labs(subtitle="Humidity for Valencia", 
       y="humidity", 
       x="# data point")  

Valencia.wind_speed <- ggplot(Valencia_Data) +
  geom_point( aes(c(1:length(wind_speed)),wind_speed)) +
  labs(subtitle="Wind Speed for Valencia", 
       y="wind_speed", 
       x="# data point")  

Valencia.pressure <- ggplot(Valencia_Data) +
  geom_point( aes(c(1:length(pressure)),pressure)) +
  labs(subtitle="Pressure of Valencia", 
       y="pressure", 
       x="# data point")  

ggarrange(Valencia.rain_duration,Valencia.humidity,Valencia.wind_speed,Valencia.pressure,
          labels = c("(A)", "(B)", "(C)","(D)"),
          ncol = 2, nrow = 2)


E_1<-Valencia_Data$energy_demand[-1] # Get all the data except in first row 
Valencia_Data<-Valencia_Data[-nrow(Valencia_Data),]
Valencia_Data$E_1 <- E_1

E_2<-E_1[-1]
Valencia_Data<-Valencia_Data[-nrow(Valencia_Data),]
Valencia_Data$E_2 <- E_2

E_25<-Valencia_Data$energy_demand[25:nrow(Valencia_Data)] # Get all the data except in first row
Valencia_Data<-Valencia_Data[1:(nrow(Valencia_Data)-24),]

Valencia_Data$E_25 <- E_25

Valencia_Data <- Valencia_Data [, c("time_ID", "E_1","E_2","E_25",   "temp","humidity","pressure","wind_speed","rain_duration","snow_duration","day_night","time_band","season","weather_main","energy_demand")]


### Generate Working and Evaluation Data
set.seed(501)
working_rows <- sample(1:nrow(Valencia_Data),0.80*nrow(Valencia_Data))

working_Valencia <-Valencia_Data[working_rows,]

working_Valencia$index <- as.numeric(row.names(working_Valencia))
working_Valencia<- working_Valencia[order(working_Valencia$index), ]

working_Valencia<-working_Valencia %>%
  relocate(index) 


evaluation_Valencia <-Valencia_Data[-working_rows,]
evaluation_Valencia$index <- as.numeric(row.names(evaluation_Valencia))
evaluation_Valencia<- evaluation_Valencia[order(evaluation_Valencia$index), ]

evaluation_Valencia<-evaluation_Valencia %>%
  relocate(index) 

write.csv(working_Valencia,"Data/working_Valencia.csv", row.names = FALSE)
write.csv(evaluation_Valencia,"Data/evaluation_Valencia.csv", row.names = FALSE)

```


## Bilbao DATA Preparation 
```{r}

Bilbao_raw<-read.csv(file = "Data/Bilbao.csv")
colnames(Bilbao_raw)[colnames(Bilbao_raw) == "X"] <- "DataID"
colnames(Bilbao_raw)[colnames(Bilbao_raw) == "dt_iso"] <- "time"
Bilbao_Data<-Bilbao_raw %>%
  dplyr::select(DataID,time,temp,humidity,pressure,wind_speed,rain_1h,rain_3h,snow_3h,weather_main,energy)

Bilbao_Data<-transform(Bilbao_Data, weather_main = factor(weather_main, 
                                                              levels = c("clear", "clouds", "drizzle","dust","fog","haze","mist","rain","smoke","snow","squall","thunderstorm"),
                                                              labels = c(1:12)))

# Add two columns of rain duration
Bilbao_Data["rain_duration"] <- Bilbao_Data$rain_1h + Bilbao_Data$rain_3h 

# Rename Column Names for clarity 
Bilbao_Data <-merge(Bilbao_Data, energy_raw[,c("time","Specified Categorical Variable:\r\nTime Band (Peak, Off-Peak, Mid-Peak)")], by.x = "time", by.y = "time")
colnames(Bilbao_Data )[colnames(Bilbao_Data ) == "Specified Categorical Variable:\r\nTime Band (Peak, Off-Peak, Mid-Peak)"] <- "time_band"

Bilbao_Data <-merge(Bilbao_Data, energy_raw[,c("time","Specified Categorical Variable:\r\nSeason (Spring, Summer, Autumn, Winter)")], by.x = "time", by.y = "time")
colnames(Bilbao_Data )[colnames(Bilbao_Data ) == "Specified Categorical Variable:\r\nSeason (Spring, Summer, Autumn, Winter)"] <- "season"

Bilbao_Data <-merge(Bilbao_Data, energy_raw[,c("time","Specified Categorical Variable:\r\nTime of day (Day vs Night)")], by.x = "time", by.y = "time")
colnames(Bilbao_Data )[colnames(Bilbao_Data ) == "Specified Categorical Variable:\r\nTime of day (Day vs Night)"] <- "day_night"


colnames(Bilbao_Data )[colnames(Bilbao_Data ) == "snow_3h"] <- "snow_duration"

Bilbao_Data<-subset(Bilbao_Data,select=-c(rain_1h,rain_3h,DataID))

colnames(Bilbao_Data )[colnames(Bilbao_Data ) == "time"] <- "time_ID"

Bilbao_Data <- Bilbao_Data [, c("time_ID","temp","humidity","pressure","wind_speed","rain_duration","snow_duration","day_night","time_band","season","weather_main","energy")]

colnames(Bilbao_Data )[colnames(Bilbao_Data ) == "energy"] <- "energy_demand"

BD_<- Bilbao_Data[Bilbao_Data$temp < 270 ,]
Bilbao_Data<- subset(Bilbao_Data, temp>270) # remove temperature less than 270K
Bilbao_Data<- subset(Bilbao_Data, pressure >900 & pressure <1050) # remove pressure outside [900,1050]mbar
Bilbao_Data<-subset(Bilbao_Data, energy_demand >10) # Remove all 0 demand from the data

Bilbao.rain_duration <- ggplot(Bilbao_Data) +
  geom_point( aes(c(1:length(rain_duration)),rain_duration)) +
  labs(subtitle="Rain Duration for Bilbao", 
       y="rain duration", 
       x="# data point") 

Bilbao.humidity <- ggplot(Bilbao_Data) +
  geom_point( aes(c(1:length(humidity)),humidity)) +
  labs(subtitle="Humidity for Bilbao", 
       y="humidity", 
       x="# data point")  

Bilbao.wind_speed <- ggplot(Bilbao_Data) +
  geom_point( aes(c(1:length(wind_speed)),wind_speed)) +
  labs(subtitle="Wind Speed for Bilbao", 
       y="wind_speed", 
       x="# data point")  

Bilbao.pressure <- ggplot(Bilbao_Data) +
  geom_point( aes(c(1:length(pressure)),pressure)) +
  labs(subtitle="Pressure of Bilbao", 
       y="pressure", 
       x="# data point")  

ggarrange(Bilbao.rain_duration,Bilbao.humidity,Bilbao.wind_speed,Bilbao.pressure,
          labels = c("(A)", "(B)", "(C)","(D)"),
          ncol = 2, nrow = 2)


E_1<-Bilbao_Data$energy_demand[-1] # Get all the data except in first row 
Bilbao_Data<-Bilbao_Data[-nrow(Bilbao_Data),]
Bilbao_Data$E_1 <- E_1

E_2<-E_1[-1]
Bilbao_Data<-Bilbao_Data[-nrow(Bilbao_Data),]
Bilbao_Data$E_2 <- E_2

E_25<-Bilbao_Data$energy_demand[25:nrow(Bilbao_Data)] # Get all the data except in first row
Bilbao_Data<-Bilbao_Data[1:(nrow(Bilbao_Data)-24),]

Bilbao_Data$E_25 <- E_25

Bilbao_Data <- Bilbao_Data [, c("time_ID", "E_1","E_2","E_25",   "temp","humidity","pressure","wind_speed","rain_duration","snow_duration","day_night","time_band","season","weather_main","energy_demand")]


### Generate Working and Evaluation Data
set.seed(501)
working_rows <- sample(1:nrow(Bilbao_Data),0.80*nrow(Bilbao_Data))

working_Bilbao <-Bilbao_Data[working_rows,]

working_Bilbao$index <- as.numeric(row.names(working_Bilbao))
working_Bilbao<- working_Bilbao[order(working_Bilbao$index), ]

working_Bilbao<-working_Bilbao %>%
  relocate(index) 


evaluation_Bilbao <-Bilbao_Data[-working_rows,]
evaluation_Bilbao$index <- as.numeric(row.names(evaluation_Bilbao))
evaluation_Bilbao<- evaluation_Bilbao[order(evaluation_Bilbao$index), ]

evaluation_Bilbao<-evaluation_Bilbao %>%
  relocate(index) 

write.csv(working_Bilbao,"Data/working_Bilbao.csv", row.names = FALSE)
write.csv(evaluation_Bilbao,"Data/evaluation_Bilbao.csv", row.names = FALSE)

```


## Seville DATA Preparation 
```{r}

Seville_raw<-read.csv(file = "Data/Seville.csv")
colnames(Seville_raw)[colnames(Seville_raw) == "X"] <- "DataID"
colnames(Seville_raw)[colnames(Seville_raw) == "dt_iso"] <- "time"
Seville_Data<-Seville_raw %>%
  dplyr::select(DataID,time,temp,humidity,pressure,wind_speed,rain_1h,rain_3h,snow_3h,weather_main,energy)

Seville_Data<-transform(Seville_Data, weather_main = factor(weather_main, 
                                                              levels = c("clear", "clouds", "drizzle","dust","fog","haze","mist","rain","smoke","snow","squall","thunderstorm"),
                                                              labels = c(1:12)))

# Add two columns of rain duration
Seville_Data["rain_duration"] <- Seville_Data$rain_1h + Seville_Data$rain_3h 

# Rename Column Names for clarity 
Seville_Data <-merge(Seville_Data, energy_raw[,c("time","Specified Categorical Variable:\r\nTime Band (Peak, Off-Peak, Mid-Peak)")], by.x = "time", by.y = "time")
colnames(Seville_Data )[colnames(Seville_Data ) == "Specified Categorical Variable:\r\nTime Band (Peak, Off-Peak, Mid-Peak)"] <- "time_band"

Seville_Data <-merge(Seville_Data, energy_raw[,c("time","Specified Categorical Variable:\r\nSeason (Spring, Summer, Autumn, Winter)")], by.x = "time", by.y = "time")
colnames(Seville_Data )[colnames(Seville_Data ) == "Specified Categorical Variable:\r\nSeason (Spring, Summer, Autumn, Winter)"] <- "season"

Seville_Data <-merge(Seville_Data, energy_raw[,c("time","Specified Categorical Variable:\r\nTime of day (Day vs Night)")], by.x = "time", by.y = "time")
colnames(Seville_Data )[colnames(Seville_Data ) == "Specified Categorical Variable:\r\nTime of day (Day vs Night)"] <- "day_night"


colnames(Seville_Data )[colnames(Seville_Data ) == "snow_3h"] <- "snow_duration"

Seville_Data<-subset(Seville_Data,select=-c(rain_1h,rain_3h,DataID))

colnames(Seville_Data )[colnames(Seville_Data ) == "time"] <- "time_ID"

Seville_Data <- Seville_Data [, c("time_ID","temp","humidity","pressure","wind_speed","rain_duration","snow_duration","day_night","time_band","season","weather_main","energy")]

colnames(Seville_Data )[colnames(Seville_Data ) == "energy"] <- "energy_demand"

SD_<-Seville_Data[Seville_Data$temp < 270 ,]
Seville_Data<- subset(Seville_Data, temp>270) # remove temperature less than 270K
Seville_Data<- subset(Seville_Data, pressure >900 & pressure <1050) # remove pressure outside [900,1050]mbar
Seville_Data<-subset(Seville_Data, energy_demand >10) # Remove all 0 demand from the data

Seville.rain_duration <- ggplot(Seville_Data) +
  geom_point( aes(c(1:length(rain_duration)),rain_duration)) +
  labs(subtitle="Rain Duration for Seville", 
       y="rain duration", 
       x="# data point") 

Seville.humidity <- ggplot(Seville_Data) +
  geom_point( aes(c(1:length(humidity)),humidity)) +
  labs(subtitle="Humidity for Seville", 
       y="humidity", 
       x="# data point")  

Seville.wind_speed <- ggplot(Seville_Data) +
  geom_point( aes(c(1:length(wind_speed)),wind_speed)) +
  labs(subtitle="Wind Speed for Seville", 
       y="wind_speed", 
       x="# data point")  

Seville.pressure <- ggplot(Seville_Data) +
  geom_point( aes(c(1:length(pressure)),pressure)) +
  labs(subtitle="Pressure of Seville", 
       y="pressure", 
       x="# data point")  

ggarrange(Seville.rain_duration,Seville.humidity,Seville.wind_speed,Seville.pressure,
          labels = c("(A)", "(B)", "(C)","(D)"),
          ncol = 2, nrow = 2)


E_1<-Seville_Data$energy_demand[-1] # Get all the data except in first row 
Seville_Data<-Seville_Data[-nrow(Seville_Data),]
Seville_Data$E_1 <- E_1

E_2<-E_1[-1]
Seville_Data<-Seville_Data[-nrow(Seville_Data),]
Seville_Data$E_2 <- E_2

E_25<-Seville_Data$energy_demand[25:nrow(Seville_Data)] # Get all the data except in first row
Seville_Data<-Seville_Data[1:(nrow(Seville_Data)-24),]

Seville_Data$E_25 <- E_25

Seville_Data <- Seville_Data [, c("time_ID", "E_1","E_2","E_25",   "temp","humidity","pressure","wind_speed","rain_duration","snow_duration","day_night","time_band","season","weather_main","energy_demand")]


### Generate Working and Evaluation Data
set.seed(501)
working_rows <- sample(1:nrow(Seville_Data),0.80*nrow(Seville_Data))

working_Seville <-Seville_Data[working_rows,]

working_Seville$index <- as.numeric(row.names(working_Seville))
working_Seville<- working_Seville[order(working_Seville$index), ]

working_Seville<-working_Seville %>%
  relocate(index) 


evaluation_Seville <-Seville_Data[-working_rows,]
evaluation_Seville$index <- as.numeric(row.names(evaluation_Seville))
evaluation_Seville<- evaluation_Seville[order(evaluation_Seville$index), ]

evaluation_Seville<-evaluation_Seville %>%
  relocate(index) 

write.csv(working_Seville,"Data/working_Seville.csv", row.names = FALSE)
write.csv(evaluation_Seville,"Data/evaluation_Seville.csv", row.names = FALSE)

```


## Madrid DATA Preparation 
```{r}

Madrid_raw<-read.csv(file = "Data/Madrid.csv")
colnames(Madrid_raw)[colnames(Madrid_raw) == "X"] <- "DataID"
colnames(Madrid_raw)[colnames(Madrid_raw) == "dt_iso"] <- "time"
Madrid_Data<-Madrid_raw %>%
  dplyr::select(DataID,time,temp,humidity,pressure,wind_speed,rain_1h,rain_3h,snow_3h,weather_main,energy)

Madrid_Data<-transform(Madrid_Data, weather_main = factor(weather_main, 
                                                              levels = c("clear", "clouds", "drizzle","dust","fog","haze","mist","rain","smoke","snow","squall","thunderstorm"),
                                                              labels = c(1:12)))

# Add two columns of rain duration
Madrid_Data["rain_duration"] <- Madrid_Data$rain_1h + Madrid_Data$rain_3h 

# Rename Column Names for clarity 
Madrid_Data <-merge(Madrid_Data, energy_raw[,c("time","Specified Categorical Variable:\r\nTime Band (Peak, Off-Peak, Mid-Peak)")], by.x = "time", by.y = "time")
colnames(Madrid_Data )[colnames(Madrid_Data ) == "Specified Categorical Variable:\r\nTime Band (Peak, Off-Peak, Mid-Peak)"] <- "time_band"

Madrid_Data <-merge(Madrid_Data, energy_raw[,c("time","Specified Categorical Variable:\r\nSeason (Spring, Summer, Autumn, Winter)")], by.x = "time", by.y = "time")
colnames(Madrid_Data )[colnames(Madrid_Data ) == "Specified Categorical Variable:\r\nSeason (Spring, Summer, Autumn, Winter)"] <- "season"

Madrid_Data <-merge(Madrid_Data, energy_raw[,c("time","Specified Categorical Variable:\r\nTime of day (Day vs Night)")], by.x = "time", by.y = "time")
colnames(Madrid_Data )[colnames(Madrid_Data ) == "Specified Categorical Variable:\r\nTime of day (Day vs Night)"] <- "day_night"


colnames(Madrid_Data )[colnames(Madrid_Data ) == "snow_3h"] <- "snow_duration"

Madrid_Data<-subset(Madrid_Data,select=-c(rain_1h,rain_3h,DataID))

colnames(Madrid_Data )[colnames(Madrid_Data ) == "time"] <- "time_ID"

Madrid_Data <- Madrid_Data [, c("time_ID","temp","humidity","pressure","wind_speed","rain_duration","snow_duration","day_night","time_band","season","weather_main","energy")]

colnames(Madrid_Data )[colnames(Madrid_Data ) == "energy"] <- "energy_demand"

MD_ <- Madrid_Data[Madrid_Data$temp < 270 ,]
Madrid_Data<- subset(Madrid_Data, temp>270) # remove temperature less than 270K
Madrid_Data<- subset(Madrid_Data, pressure >900 & pressure <1050) # remove pressure outside [900,1050]mbar
Madrid_Data<-subset(Madrid_Data, energy_demand >10) # Remove all 0 demand from the data

Madrid.rain_duration <- ggplot(Madrid_Data) +
  geom_point( aes(c(1:length(rain_duration)),rain_duration)) +
  labs(subtitle="Rain Duration for Madrid", 
       y="rain duration", 
       x="# data point") 

Madrid.humidity <- ggplot(Madrid_Data) +
  geom_point( aes(c(1:length(humidity)),humidity)) +
  labs(subtitle="Humidity for Madrid", 
       y="humidity", 
       x="# data point")  

Madrid.wind_speed <- ggplot(Madrid_Data) +
  geom_point( aes(c(1:length(wind_speed)),wind_speed)) +
  labs(subtitle="Wind Speed for Madrid", 
       y="wind_speed", 
       x="# data point")  

Madrid.pressure <- ggplot(Madrid_Data) +
  geom_point( aes(c(1:length(pressure)),pressure)) +
  labs(subtitle="Pressure of Madrid", 
       y="pressure", 
       x="# data point")  

ggarrange(Madrid.rain_duration,Madrid.humidity,Madrid.wind_speed,Madrid.pressure,
          labels = c("(A)", "(B)", "(C)","(D)"),
          ncol = 2, nrow = 2)


E_1<-Madrid_Data$energy_demand[-1] # Get all the data except in first row 
Madrid_Data<-Madrid_Data[-nrow(Madrid_Data),]
Madrid_Data$E_1 <- E_1

E_2<-E_1[-1]
Madrid_Data<-Madrid_Data[-nrow(Madrid_Data),]
Madrid_Data$E_2 <- E_2

E_25<-Madrid_Data$energy_demand[25:nrow(Madrid_Data)] # Get all the data except in first row
Madrid_Data<-Madrid_Data[1:(nrow(Madrid_Data)-24),]

Madrid_Data$E_25 <- E_25

Madrid_Data <- Madrid_Data [, c("time_ID", "E_1","E_2","E_25",   "temp","humidity","pressure","wind_speed","rain_duration","snow_duration","day_night","time_band","season","weather_main","energy_demand")]


### Generate Working and Evaluation Data
set.seed(501)
working_rows <- sample(1:nrow(Madrid_Data),0.80*nrow(Madrid_Data))

working_Madrid <-Madrid_Data[working_rows,]

working_Madrid$index <- as.numeric(row.names(working_Madrid))
working_Madrid<- working_Madrid[order(working_Madrid$index), ]

working_Madrid<-working_Madrid %>%
  relocate(index) 


evaluation_Madrid <-Madrid_Data[-working_rows,]
evaluation_Madrid$index <- as.numeric(row.names(evaluation_Madrid))
evaluation_Madrid<- evaluation_Madrid[order(evaluation_Madrid$index), ]

evaluation_Madrid<-evaluation_Madrid %>%
  relocate(index) 

write.csv(working_Madrid,"Data/working_Madrid.csv", row.names = FALSE)
write.csv(evaluation_Madrid,"Data/evaluation_Madrid.csv", row.names = FALSE)

```

# MLR Modelling



## Barcelona Model

### Data Processing

#### Read working and evaluation data set, moreover, remove the index and time stamp and snow_duration variable as these are all invalid variables (the latter as all entries are zero)

```{r}
evaldata<-read.csv('data/evaluation_Barcelona.csv')
workdata<-read.csv('data/working_Barcelona.csv')
#remove index, time_ID and snow_duration columns from "eval" analysis data frame
evalBarcelona<-evaldata[c(-1,-2,-11)] 
#remove index, time_ID and snow_duration columns from "work" analysis data frame
workBarcelona<-workdata[c(-1,-2,-11)] 
# head(workBarcelona, 4L)
```


### Create the ggpair plot and Correlation plot

```{r,,fig.height=5}
# Correlation matrix for numeric predictors only (categorical predictors excluded)
workcor<-workBarcelona[c(-9:-12) ]
ggpairs(workcor)
cor(workBarcelona)
```

### Create color spectrum correlation plot

```{r,fig.height=5}
pdf("corplot.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
corrplot(cor(workcor), method="color", type="full", addCoef.col = "red", tl.col="black",number.cex = 0.75)
dev.off()
corrplot(cor(workcor), method="color", type="full", addCoef.col = "red", tl.col="black",number.cex = 0.75)
```

## Model Processing

### Build the full model

```{r}
workModel1 <- lm(energy_demand ~ E_1 + E_2 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workBarcelona)
```

### Check the multicollinearity assumption

```{r, fig.height=4}
vif(workModel1)
multicol_<-vif(workModel1)
print("VIF values greater than 10, implying serious multicollinearity, are:")
multicol_[multicol_[,"GVIF"]>10,1]
```

According to the VIF output E_1 and E_2 variables have serious multicollinearity issue  as thus remove E_2 and rebuild the model.

### Drop E_2 to address multicollinearity between E_1 and E_2, and rebuild the model 

```{r}
workModelFull <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workBarcelona)
vif(workModelFull)
```

According to the VIF output, we can observe that there's no serious multicollinearity issue. 

### Model assumption check

```{r, fig.height=4}
# to obtain residuals
res.fullmodel1 <- residuals(workModelFull) 
# to obtain standardized residuals
std.res.fullmodel1 <- rstandard(workModelFull) 
# to obtain fitted/predicted values
pred.fullmodel1 <- fitted.values(workModelFull) 
par(mfrow=c(1,1))
qqnorm(y = std.res.fullmodel1, main = " Normal Q-Q Plot ",
       xlab = "Theoretical Quantiles", ylab = "Sample Quantiles")
qqline(y = std.res.fullmodel1)
resplotdata1 <- data.frame(std.res.fullmodel1, pred.fullmodel1)
resbf1 <- lm(std.res.fullmodel1 ~ pred.fullmodel1, data = resplotdata1)
plot(x = pred.fullmodel1, y = std.res.fullmodel1, ylab = "Standardized Residuals", xlab = "Predicted Values", main = "Residuals Plot", col = ifelse(std.res.fullmodel1 < -3,"red",ifelse(std.res.fullmodel1 > 3,"red","black")))
abline(h = 0, col="blue", lty=1)
abline(resbf1, col="red", lty=3)
abline(h = 3, col="green", lty=3)
abline(h=-3, col="green", lty=3)
legend("bottomleft", legend=c("Best fit line of standardized residuals", "Horizontal line y = 0.0", "Horizontal line, y = +/- 3"), fill = c("red","blue","green"), cex = 1.0)
```

According to the Normal QQ plot, we can observe that most of points align on the reference line thus it follows the normal distribution assumption.

According to the residual plot, we can observe that there's no heteroscedasticity issue , nevertheless, there are numerous outliers.

### Check the regression output and build the reduce model 

#### Based on regression summary, predictor *rain_duration* dropped as insignificant (p-values is > 0.05).  Build a reduce model then compare with the full model

```{r}
summary(workModelFull)
df<- workModelFull %>% 
  tidy()
#Get the variables which there p-value larger than 0.05 
df%>%
  filter(df$p.value>0.05)
workModelPvalue <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workBarcelona)
summary(workModelPvalue)
```

### Reduced model comparisons

+ p-value approach

```{r}
anova(workModelPvalue, workModelFull)
```

At 5% significance level , no statistical evidence to support that the full model is preferred over than reduced model.(it's simpler... no prediction performance is lost via the elimination of identified predictors). That is to say, workModelPvalue is preferred over workModelFull.

 
+ Best subset selection method

#### Based on the the Best subset selection method, we drop pressue, wind_speed, rain_duration, time_band and weather_main to build the reduce model.

```{r,fig.height=4}
bestfits <- regsubsets(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workBarcelona, nbest = 1)
plot(bestfits, scale="adjr2")
workModelBestfit<-lm(energy_demand ~ E_1 + E_25 + temp + humidity + factor(day_night) + factor(season) , data = workBarcelona)
summary(workModelBestfit)
anova(workModelBestfit, workModelPvalue)
```

At 5% significance level , no statistical evidence to support that the Bestfit model is preferred over than Pvalue model.(it's simpler... no prediction performance is lost via the elimination of identified predictors). That is to say, workModelPvalue is preferred over workModelBestfit.

+ AIC with forward selection approach test method

```{r}
workNullModel <- lm(energy_demand ~ 1, data = workBarcelona)
step.working <- stepAIC(workNullModel, scope = list(lower = workNullModel,
upper = workModelFull), direction = "forward", trace=FALSE)
summary(step.working)
```

### According to the AIC method output, we get the same multiple linear model as full model.

+ Cross Validation test method

```{r}
# Set seed for reproducibility in CV
set.seed(123)
# Set up repeated k-fold cross-validation, with k=10
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.cv.work <- train(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workBarcelona, method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:11),
trControl = train.control)
step.cv.work$results
summary(step.cv.work$finalModel)
workModelCross <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workBarcelona)
anova( workModelCross, workModelPvalue)
summary(workModelCross)
```

At 5% significance level , no statistical evidence to support that the Cross Validation model is preferred over than Pvalue model. (Latter is simpler... no prediction performance is lost via the elimination of identified predictors). That is to say, workModelPvalue is preferred over workModelCross

+ OLS method

```{r fig.height=4}
#ModelOLS <- ols_step_all_possible(workModelFull)
#cat("\n")
#plot(x = ModelOLS$n, y = ModelOLS$adjr)
#plot(x = ModelOLS$n, y = ModelOLS$aic)
#OLSsummary <- ModelOLS %>% group_by(n) %>%  filter(adjr == max(adjr))
#OLSsummary
```

Based on Adjusted R-square, the most favorable OLS model is equivalent to workModelFull.

## Select the final model

```{r}
Method <- c('Full', 'P-value', 'BestFits', 'AIC Forward', 'Cross Validation','OLS_step')
Variables <- c(12,11,7,12,10,12)
Coefficients <- c(22,21,9,22,20,21)
Adjusted_R2 <- c(0.9155554,0.9155523,0.9151636,0.9155554,0.9155113,0.9155523)
BarcelonaTable<-data.frame(Method,Variables,Coefficients,Adjusted_R2)
# Summary table for different selection methods
print(BarcelonaTable)
#Alternate order
#ARSlist <- list(FullARS = c(summary(workModelFull)$adj.r.squared),
#                PvalueARS = c(summary(workModelPvalue)$adj.r.squared),
#                BestfitsARS = c(summary(workModelBestfit)$adj.r.squared),
#                AICARS = c(summary(step.working)$adj.r.squared),
#                CrossvARS = c(summary(workModelCross)$adj.r.squared),
#                OLSARS = max(OLSsummary$adjr))
#ModelOrder <- as.data.frame(ARSlist) %>% pivot_longer(cols = 1:4) %>% arrange(desc(value))
#ModelOrder
```

Model Adjusted R-square from above methods (to the ten thousandths decimal place)

A) Full: 0.9156

B) P-value: 0.9156 (viewed to be the same as Full)

C) Bestfits: 0.9152

D) AIC Method: 0.9156

E) Cross-validation: 0.9155

F) OLS-step: 0.9156 (viewed to be the same as P-value)

**Final model is the workModelPvalue as it has the highest Adjusted R-square and fewest predictors.**

## Build the Final Model

```{r}
workFinal <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workBarcelona)
```


### Check the linearity, heteroscedasticity, outlier and normal assumption

```{r, fig.height=4}
par(mfrow=c(2,2))
#Residual Plot
p1<-plot(x=fitted(workFinal), y=studres(workFinal), xlab = "Fitted Values",
ylab = "Studentized Residuals", main='Residuals Plot', col = ifelse(studres(workFinal) < -3,"red",ifelse(studres(workFinal) > 3,"red","black")))
abline(h=-3, col="red", lty=2)
abline(h=3, col="red", lty=2)
#norm test
std.res <- rstandard(workFinal)
ad.test(std.res)
p2<-qqnorm(studres(workFinal), pch = 1, frame = FALSE, main=expression("AD test =2.2*10"^-16))
qqline(studres(workFinal), col = "steelblue", lwd = 2)
#leverage 
lev<-hatvalues(workFinal)
cutlev = (2*length(coefficients(workFinal)))/nrow(workBarcelona)
# Count and assess the number of leverage values > cut-off
potoutlier <- sum(lev > cutlev, na.rm=TRUE)
totcount = length(fitted(workFinal))
print(paste("The number of leverage points that are potential outliers is", potoutlier,".  This is", round(100*potoutlier/totcount,2),"% of the total number of predicted values, (", totcount,") thus not material."))
#barplot(lev, ylim = c(0, 2*cutlev))
p3<-plot(lev, type="h", ylim = c(0, 2*cutlev), ylab="Leverage for Barcelona working set")
abline(h = cutlev, col = "red", lty=2)
#cook distance
cookdist<-cooks.distance(workFinal)
#barplot(cookdist, ylim=c(0,1.01), main = "Cook's Distance plot")
#barplot(cookdist, ylim=c(0,0.005), main = "Cook's Distance plot")
#plot(log(cookdist), type="h", ylim=c(0,1.01), main = "Cook's Distance plot")
#abline(h = 1, col = "red")

ggplot(as.data.frame(cookdist),aes(x=1:nrow(as.data.frame(cookdist)),y=cookdist)) +
  geom_line() + 
  geom_hline(aes(yintercept = 1,color = "red"))+
  scale_y_continuous(limits = c(0,1.1))+
  scale_y_break(breaks = c(0.005,0.9), scales =0.1) +
  guides(color = "none") +
  labs(title="Cook's Distance",
        x ="Index", y = "")

```

Prepare summary plot for report
```{r}
# Opening the graphical device
# Customizing the output
pdf("BarcelonaAssumptionCheck1.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   
# par(mfrow=c(2,2))
#residuals scatter plot
plot(x=fitted(workFinal), y=studres(workFinal), xlab = "Fitted Values",
ylab = "Studentized Residuals", main='Residuals Plot', col = ifelse(studres(workFinal) < -3,"red",ifelse(studres(workFinal) > 3,"red","black")))
abline(h=-3, col="red", lty=2)
abline(h=3, col="red", lty=2)
grid(10,10)
dev.off()


#normality plot
pdf("BarcelonaAssumptionCheck2.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
qqnorm(studres(workFinal), pch = 1, frame = FALSE, main=expression("AD test =2.2*10"^-16))
qqline(studres(workFinal), col = "steelblue", lwd = 2)
grid(10,10)
dev.off()


#leverage plot
pdf("BarcelonaAssumptionCheck3.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model

plot(lev, type="h", ylim = c(0, 2*cutlev), ylab="Leverage for Barcelona working set")
abline(h = cutlev, col = "red", lty=2)
grid(10,10)
dev.off()


#Cook's distance plot
pdf("BarcelonaAssumptionCheck4.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
ggplot(as.data.frame(cookdist),aes(x=1:nrow(as.data.frame(cookdist)),y=cookdist)) +
  geom_line() + 
  geom_hline(aes(yintercept = 1,color = "red"))+
  scale_y_continuous(limits = c(0,1.1))+
  scale_y_break(breaks = c(0.005,0.9), scales =0.1) +
  guides(color = "none") +
  labs(title="Cook's Distance",
        x ="Index", y = "")
dev.off()

```



## Use the estimated model to predict the values in the evaluation set
```{r, fig.height=4}
newdata <- evalBarcelona[ c(-2,-13)]
predict.eval <- predict(workFinal, newdata)
plot(x = evalBarcelona$energy_demand, y=predict.eval, xlab="Actual energy demand for the evaluation set",
ylab="Predicted energy demand for the evaluation set", main = "Barcelona")
abline(a=0, b=1, col="blue")
grid(10,10)
```

Prepare report figure
```{r fig.height=4}
# Opening the graphical device
# Customizing the output
pdf("BarcelonaEvalActComp.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   
par(mfrow=c(1,1))
plot(x = evalBarcelona$energy_demand, y=predict.eval, xlab="Actual energy demand for the evaluation set",
ylab="Predicted energy demand for the evaluation set", main = "Barcelona", cex.lab = 0.75)
abline(a=0, b=1, col="blue")
grid(10,10)

# Closing the graphical device
dev.off()
```

### Compute the RMSE for the predictions

```{r}
rootmse <- rmse(evaldata$energy_demand, predict.eval)
print(paste("The root mean square error of the predicted vs actual",nrow(evaldata),"hourly energy_data points for Barcelona over the 2015-2018 timespan is an energy demand difference of",round(rootmse,2),"MWh."))
cat("\n")
demandcompdf <- data.frame("Actual (MWh)" = evalBarcelona$energy_demand,"Predicted (MWh)" = predict.eval)
# head(demandcompdf,4L)
cat("\n")
print(paste("For example, the first actual demand value is",round(evaldata$energy_demand[1],2),"MWh and the corresponding predicted value is",round(predict.eval[1],2),"MWh. The percent error between this predicted value relative to the actual value is",round((100*(predict.eval[1] - evaldata$energy_demand[1])/evaldata$energy_demand[1]),2),"%."))
cat("\n")
diffpct <- c((abs(evaldata$energy_demand - predict.eval))/evaldata$energy_demand)
print(paste("The mean difference between the",nrow(evaldata),"evaluation data set actual and model predicted values is",round(100*mean(diffpct),2),"%."))

```


### Compute and analyze Hat Matrix for Evaluation data set
Obtain X matrix from the model
```{r}
Predictors <-rownames(summary(workFinal)$coefficients[,])
workpredictors <- as.data.frame(Predictors)
X <- model.matrix(workFinal)
class(X)
cat("\n")
dim(X)
cat("\n")
#head(X, 4L)
```

### Create x_new matrix using the validation data.  Note that the purpose of running the evaluation regression here is to extract all of the correct predictor coefficients.
```{r}
evalFinal <- lm(energy_demand ~ E_1 + E_25 + temp + humidity 
                + pressure + wind_speed + factor(day_night) 
                + factor(time_band) + factor(season) 
                + factor(weather_main), data = evalBarcelona)
Predictors <-rownames(summary(evalFinal)$coefficients[,])
evalpredictors <- as.data.frame(Predictors)
x_new1 <- model.matrix(evalFinal)
cat("\n")
dim(x_new1)
cat("\n")
#Check if there's a predictor difference between the work and eval matrices
workevalcompare <-anti_join(workpredictors,evalpredictors)
cat("\n")
```
### When workevalcompare is not Null, run the following lines of code after identifying which column is missing between X and x_new1 (for Barcelona, it was found that "factor(weather_main)4" appears in WorkBarcelona but not in evalBarcelona).

```{r}
x_new_ <-as.data.frame(cbind(x_new1,rep(0,nrow(x_new1))))
dummy_xnew<- x_new_ %>% relocate(V21,.before = 16) %>% rename("factor(weather_main)4" = V21)
x_new <- as.matrix(dummy_xnew)  
cat("\n")
class(x_new)
cat("\n")
dim(x_new)
cat("\n")
# head(x_new, 4L)
```


### Compute the hat values for the validation data
```{r}
h_new_mid <- solve(t(X)%*%X)
dim(h_new_mid)
h_new <- x_new%*%h_new_mid%*%t(x_new)
dim(h_new)
```

### Plot the hatvalues for the data in the evaluation set
```{r fig.height=4}
#cutlev2 = (2*length(coefficients(evalFinal)))/nrow(evalBarcelona)
# Count and assess the number of leverage values > cut-off
potoutlier2 <- sum(diag(h_new) > cutlev, na.rm=TRUE)
totcount2 = nrow(evalBarcelona)
plot(diag(h_new), type = "h", ylab = "Leverage for the validation data set", ylim = c(0,2*cutlev), main = "Barcelona")
abline(h=cutlev, col="red", lty=2)
print(paste("The number of leverage points that are potential outliers is", potoutlier2,".  This is", round(100*potoutlier2/totcount2,2),"% of the total number of predicted values, (", totcount2,") thus not material."))
```

### Side-by-side comparision
```{r fig.height=4}
par(mfrow=c(1,2))
plot(lev, type="h", ylim = c(0, 2*cutlev), ylab="Leverage for the working data set", main = "Barcelona")
abline(h = cutlev, col = "red", lty=2)
plot(diag(h_new), type = "h", ylim = c(0,2*cutlev), ylab = "Leverage for the evaluation data set", main = "Barcelona")
abline(h=cutlev, col="red", lty=2)
print(paste("The leverage cutoff exceedences in the evaluation set is",round(100*potoutlier2/totcount2,2),"% of the total number of observed values (", totcount2,") and is comparable to that of the working set,", round(100*potoutlier/totcount,2),"% of the total number of predicted values (", totcount,")."))
```

Prepare report figure
```{r}
# Opening the graphical device
# Customizing the output
pdf("BarcelonaLevComp.pdf",         # File name
    width = 14, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   
par(mfrow=c(1,2))
plot(lev, type="h", ylab = "Leverage", ylim = c(0,2*cutlev), yaxt = 'n', main = "Barcelona Working Data", res =600)
abline(h = cutlev, col = "red", lty=2)
axis(side = 2, at=seq(0,2*round(cutlev,5),by=round(cutlev,5)))

plot(diag(h_new), type = "h", ylab = "", ylim = c(0,2*cutlev), yaxt = 'n', main = "Barcelona Evaluation Data")
abline(h=cutlev, col="red", lty=2)
axis(side = 2, at=seq(0,2*round(cutlev,5),by=round(cutlev,5)))

# Closing the graphical device
dev.off()
```


## Bilbao Model

```{r}
evaldata<-read.csv('data/evaluation_Bilbao.csv')
workdata<-read.csv('data/working_Bilbao.csv')
#remove index and time_ID columns from "eval" analysis data frame
evalBilbao<-evaldata[c(-1,-2)] 
#remove index and time_ID columns from "work" analysis data frame
workBilbao<-workdata[c(-1,-2)] 
# head(workBilbao, 4L)
```


```{r,,fig.height=5}
# Correlation matrix for numeric predictors only (categorical predictors excluded)
workcor<-workBilbao[c(-10:-13)]
workcor_<-ggpairs(workcor)
corvalues<- cor(workBilbao)
# corplot2<- corrplot(cor(workcor), method="color", type="full", addCoef.col = "red", tl.col="black",number.cex = 0.75)
```


## Model Processing

```{r}
workModelBilbao1 <- lm(energy_demand ~ E_1 + E_2 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + snow_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workBilbao)
```


```{r, fig.height=4}
vif_values <- vif(workModelBilbao1)
```


```{r}
workModelBilbaoFull <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + snow_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workBilbao)
vif_values2<- vif(workModelBilbaoFull)
```


```{r, fig.height=5}
# to obtain residuals
res.fullmodel1 <- residuals(workModelBilbaoFull) 
# to obtain standardized residuals
std.res.fullmodel1 <- rstandard(workModelBilbaoFull) 
# to obtain fitted/predicted values
pred.fullmodel1 <- fitted.values(workModelBilbaoFull) 
par(mfrow=c(1,1))
qqnorm(y = std.res.fullmodel1, main = " Normal Q-Q Plot ",
       xlab = "Theoretical Quantiles", ylab = "Sample Quantiles")
qqline(y = std.res.fullmodel1)
#residual plot
resplotdata1 <- data.frame(std.res.fullmodel1, pred.fullmodel1)
resbf1 <- lm(std.res.fullmodel1 ~ pred.fullmodel1, data = resplotdata1)
plot(x = pred.fullmodel1, y = std.res.fullmodel1, ylab = "Standardized Residuals", xlab = "Predicted Values", main = "Residuals Plot", col = ifelse(std.res.fullmodel1 < -3,"red",ifelse(std.res.fullmodel1 > 3,"red","black")))
abline(h = 0, col="blue", lty=1)
abline(resbf1, col="red", lty=3)
abline(h = 3, col="green", lty=3)
abline(h=-3, col="green", lty=3)
legend("bottomleft", legend=c("Best fit line of standardized residuals", "Horizontal line y = 0.0", "Horizontal line, y = +/- 3"), fill = c("red","blue","green"), cex = 1.0)
```

```{r}
# summary(workModelBilbaoFull)
df<- workModelBilbaoFull %>% 
  tidy()
#Get the variables which there p-value larger than 0.05 
df%>%
  filter(df$p.value>0.05)
workModelBilbaoPvalue <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workBilbao)
#summary(workModelBilbaoPvalue)
```

### Reduced model comparisons

+ p-value approach

```{r}
anovacheck1<-anova(workModelBilbaoPvalue, workModelBilbaoFull)
```

+ Best subset selection method

```{r, fig.height=5}
bestfits <- regsubsets(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + snow_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workBilbao, nbest = 1)
# plot(bestfits, scale="adjr2")
```

```{r}
workModelBilbaoBestfit<-lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + rain_duration + factor(day_night) + factor(weather_main), data = workBilbao)
# summary(workModelBilbaoBestfit)
anovacheck2<-anova(workModelBilbaoBestfit, workModelBilbaoPvalue)
```

+ AIC with forward selection approach test method

```{r}
workNullModel<- lm(energy_demand ~ 1, data = workBilbao)
step.working <- stepAIC(workNullModel, scope = list(lower = workNullModel,
upper = workModelBilbaoFull), direction = "forward", trace=FALSE)
# summary(step.working)
```

Same reduced model as P-Value approach

+ Cross Validation test method

```{r}
# Set seed for reproducibility in CV
set.seed(123)
# Set up repeated k-fold cross-validation, with k=10
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.cv.work <- train(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + snow_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workBilbao, method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:12),
trControl = train.control)
# step.cv.work$results
# summary(step.cv.work$finalModel)
```

```{r}
workModelBilbaoCross <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + rain_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workBilbao)
anovacheck3<-anova( workModelBilbaoCross, workModelBilbaoPvalue)
# summary(workModelBilbaoCross)
```

+ OLS method

```{r fig.height=4}
#ModelOLS <- ols_step_all_possible(workModelBilbaoFull)
#cat("\n")
#plot(x = ModelOLS$n, y = ModelOLS$adjr)
#plot(x = ModelOLS$n, y = ModelOLS$aic)
#OLSsummary <- ModelOLS %>% group_by(n) %>%  filter(adjr == max(adjr))
#cat("\n")
#OLSsummary
```

Same reduced model as P-Value approach (thus commenting out given processing time)

## Select the final model

Model adjusted R-square from above methods (to the ten thousandths decimal place)

A) Full: 0.9154

B) P-value: 0.9154

C) Bestfits: 0.9149

D) AIC Method: 0.9154

E) Cross-validation: 0.9154

F) OLS-step: 0.9154

**Final model is the workModelBilbaoPvalue as it has the highest Adjusted R-square and fewest predictors.**

```{r}
Method <- c('Full', 'P-value', 'BestFits', 'AIC Forward', 'Cross Validation','OLS_step')
Variables <- c(13,12,9,12,11,12)
Coefficients <- c(23,22,16,22,21,22)
Adjusted_R2 <- c(0.9153865,0.9153886,0.9148913,0.9153886,0.9153680,0.9153886)
BilbaoTable<-data.frame(Method,Variables,Coefficients,Adjusted_R2)
```

### Build the Final Model

```{r}
workFinal <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workBilbao)
```


### Check the linearity, heteroscedasticity, outlier and normal assumption

```{r, fig.height=4}
par(mfrow=c(2,2))
#Residual Plot
plot(x=fitted(workFinal), y=studres(workFinal), xlab = "Fitted Values",
ylab = "Studentized Residuals", main='Residuals Plot', col = ifelse(studres(workFinal) < -3,"red",ifelse(studres(workFinal) > 3,"red","black")))
abline(h=-3, col="red", lty=2)
abline(h=3, col="red", lty=2)
#norm test
std.res <- rstandard(workFinal)
ad.test(std.res)
qqnorm(studres(workFinal), pch = 1, frame = FALSE, main=expression("AD test =2.2*10"^-16))
qqline(studres(workFinal), col = "steelblue", lwd = 2)
#leverage 
lev<-hatvalues(workFinal)
cutlev = (2*length(coefficients(workFinal)))/nrow(workBilbao)
# Count and assess the number of leverage values > cut-off
potoutlier <- sum(lev > cutlev, na.rm=TRUE)
totcount = length(fitted(workFinal))
print(paste("The number of leverage points that are potential outliers is", potoutlier,".  This is", round(100*potoutlier/totcount,2),"% of the total number of predicted values, (", totcount,") thus not material."))
#barplot(lev, ylim = c(0, 2*cutlev))
plot(lev, type="h", ylim = c(0, 2*cutlev), ylab="Leverage for Bilbao working set")
abline(h = cutlev, col = "red", lty=2)
#cook distance
cookdist<-cooks.distance(workFinal)
#barplot(cookdist, ylim=c(0,1.01), main = "Cook's Distance plot")
#barplot(cookdist, ylim=c(0,0.005), main = "Cook's Distance plot")
#plot((cookdist), type="h", ylim=c(0,0.05), main = "Cook's Distance plot")
#abline(h = 1, col = "red")
ggplot(as.data.frame(cookdist),aes(x=1:nrow(as.data.frame(cookdist)),y=cookdist)) +
  geom_line() + 
  geom_hline(aes(yintercept = 1,color = "red"))+
  scale_y_continuous(limits = c(0,1.1))+
  scale_y_break(breaks = c(0.015,0.9), scales =0.1) +
  guides(color = "none") +
  labs(title="Cook's Distance",
        x ="Index", y = "")
```

Prepare summary plot for report
```{r}
# Opening the graphical device
# Customizing the output
pdf("BilbaoAssumptionCheck1.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   
# par(mfrow=c(2,2))
#residuals scatter plot
plot(x=fitted(workFinal), y=studres(workFinal), xlab = "Fitted Values",
ylab = "Studentized Residuals", main='Residuals Plot', col = ifelse(studres(workFinal) < -3,"red",ifelse(studres(workFinal) > 3,"red","black")))
abline(h=-3, col="red", lty=2)
abline(h=3, col="red", lty=2)
grid(10,10)
dev.off()


#normality plot
pdf("BilbaoAssumptionCheck2.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
qqnorm(studres(workFinal), pch = 1, frame = FALSE, main=expression("AD test =2.2*10"^-16))
qqline(studres(workFinal), col = "steelblue", lwd = 2)
grid(10,10)
dev.off()


#leverage plot
pdf("BilbaoAssumptionCheck3.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model

plot(lev, type="h", ylim = c(0, 2*cutlev), ylab="Leverage for Barcelona working set")
abline(h = cutlev, col = "red", lty=2)
grid(10,10)
dev.off()


#Cook's distance plot
pdf("BilbaoAssumptionCheck4.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
ggplot(as.data.frame(cookdist),aes(x=1:nrow(as.data.frame(cookdist)),y=cookdist)) +
  geom_line() + 
  geom_hline(aes(yintercept = 1,color = "red"))+
  scale_y_continuous(limits = c(0,1.1))+
  scale_y_break(breaks = c(0.005,0.9), scales =0.1) +
  guides(color = "none") +
  labs(title="Cook's Distance",
        x ="Index", y = "")
dev.off()

```






### Use the estimated model to predict the values in the evaluation set

```{r fig.height=4}
newdata <- evalBilbao[ c(-2,-14)]
predict.eval <- predict(workFinal, newdata)
plot(x = evalBilbao$energy_demand, y=predict.eval, xlab="Actual energy demand for the evaluation set",
ylab="Predicted energy demand for the evaluation set", main = "Bilbao")
abline(a=0, b=1, col="blue")
grid(10,10)
```

Prepare report figure
```{r fig.height=4}
# Opening the graphical device
# Customizing the output
pdf("BilbaoEvalActComp.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   
par(mfrow=c(1,1))
plot(x = evalBilbao$energy_demand, y=predict.eval, xlab="Actual energy demand for the evaluation set",
ylab="Predicted energy demand for the evaluation set", main = "Bilbao", cex.lab = 0.75)
abline(a=0, b=1, col="blue")
grid(10,10)

# Closing the graphical device
dev.off()
```

### Compute the RMSE for the predictions

```{r}
rootmse <- rmse(evalBilbao$energy_demand, predict.eval)
print(paste("The root mean square error of the predicted vs actual",nrow(evaldata),"hourly energy_data points for Bilbao over the 2015-2018 timespan is an energy demand difference of",round(rootmse,2),"MWh."))
cat("\n")
demandcompdf <- data.frame("Actual (MWh)" = evalBilbao$energy_demand,"Predicted (MWh)" = predict.eval)
# head(demandcompdf,4L)
cat("\n")
print(paste("For example, the first actual demand value is",round(evaldata$energy_demand[1],2),"MWh and the corresponding predicted value is",round(predict.eval[1],2),"MWh. The percent error between this predicted value relative to the actual value is",round((100*(predict.eval[1] - evaldata$energy_demand[1])/evaldata$energy_demand[1]),2),"%."))
cat("\n")
diffpct <- c((abs(evaldata$energy_demand - predict.eval))/evaldata$energy_demand)
print(paste("The mean difference between the",nrow(evaldata),"evaluation data set actual and model predicted values is",round(100*mean(diffpct),2),"%."))
```

## Compute and analyze Hat Matrix for Evaluation data set
Obtain X matrix from the model
```{r}
Predictors <-rownames(summary(workFinal)$coefficients[,])
workpredictors <- as.data.frame(Predictors)
X <- model.matrix(workFinal)
class(X)
cat("\n")
dim(X)
cat("\n")
# head(X, 4L)
```

Create x_new matrix using the validation data.  Note that the purpose of running the evaluation regression here is to extract all of the correct predictor coefficients.
```{r}
evalFinal <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = evalBilbao)
Predictors <-rownames(summary(evalFinal)$coefficients[,])
evalpredictors <- as.data.frame(Predictors)
x_new1 <- model.matrix(evalFinal)
cat("\n")
dim(x_new1)
cat("\n")
#Check if there's a predictor difference between the work and eval matrices
workevalcompare <-anti_join(workpredictors,evalpredictors)
```

### If workevalcompare is 'No data available in table', run the next line of code and skip to line 816
```{r}
x_new <- x_new1
```


### Otherwise when workevalcompare is not 'No data available in table', run the following lines of code after identifying which column is missing between X and x_new1 (no discrepency was found between workBilbao but not in evalBilbao).

```{r}
#x_new_ <-as.data.frame(cbind(x_new1,rep(0,nrow(x_new1))))
#dummy_xnew<- x_new_ %>% relocate(V21,.before = 16) %>% rename("factor(weather_main)4" = V21)
#x_new <- as.matrix(dummy_xnew)  
cat("\n")
class(x_new)
cat("\n")
dim(x_new)
cat("\n")
# head(x_new, 4L)
```




### Compute the hat values for the validation data
```{r}
h_new_mid <- solve(t(X)%*%X)
dim(h_new_mid)
h_new <- x_new%*%h_new_mid%*%t(x_new)
dim(h_new)
```

Plot the hatvalues for the data in the evaluation set
```{r fig.height=4}
#cutlev2 = (2*length(coefficients(evalFinal)))/nrow(evalBarcelona)
# Count and assess the number of leverage values > cut-off
potoutlier2 <- sum(diag(h_new) > cutlev, na.rm=TRUE)
totcount2 = nrow(evalBilbao)
# plot(diag(h_new), type = "h", ylab = "Leverage for the validation data set", ylim = c(0,2*cutlev), main = "Bilbao")
# abline(h=cutlev, col="red", lty=2)
print(paste("The number of leverage points that are potential outliers is", potoutlier2,".  This is", round(100*potoutlier2/totcount2,2),"% of the total number of predicted values, (", totcount2,") thus not material."))
```

Side-by-side comparision
```{r fig.height=4}
par(mfrow=c(1,2))
plot(lev, type="h", ylim = c(0, 2*cutlev), ylab="Leverage for the working data set", main = "Barcelona")
abline(h = cutlev, col = "red", lty=2)
plot(diag(h_new), type = "h", ylim = c(0,2*cutlev), ylab = "Leverage for the evaluation data set", main = "Barcelona")
abline(h=cutlev, col="red", lty=2)
print(paste("The leverage cutoff exceedences in the evaluation set is",round(100*potoutlier2/totcount2,2),"% of the total number of observed values (", totcount2,") and is comparable to that of the working set,", round(100*potoutlier/totcount,2),"% of the total number of predicted values (", totcount,")."))
```

Prepare report figure
```{r}
# Opening the graphical device
# Customizing the output
pdf("BilbaoLevComp.pdf",         # File name
    width = 14, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   
par(mfrow=c(1,2))
plot(lev, type="h", ylab = "Leverage", ylim = c(0,2*cutlev), yaxt = 'n', main = "Bilbao Working Data", res =600)
abline(h = cutlev, col = "red", lty=2)
axis(side = 2, at=seq(0,2*round(cutlev,5),by=round(cutlev,5)))

plot(diag(h_new), type = "h", ylab = "", ylim = c(0,2*cutlev), yaxt = 'n', main = "Bilbao Evaluation Data")
abline(h=cutlev, col="red", lty=2)
axis(side = 2, at=seq(0,2*round(cutlev,5),by=round(cutlev,5)))

# Closing the graphical device
dev.off()
```

## Madrid Model

```{r}
evaldata<-read.csv('data/evaluation_Madrid.csv')
workdata<-read.csv('data/working_Madrid.csv')
#remove index and time_ID columns from "eval" analysis dataframe
evalMadrid<-evaldata[c(-1,-2)] 
#remove index and time_ID columns from "work" analysis dataframe
workMadrid<-workdata[c(-1,-2)] 
head(workMadrid, 4L)
```


```{r,,fig.height=5}
# Correlation matrix for numeric predictors only (categorical predictors excluded)
workcor<-workMadrid[c(-10:-13) ]
ggpairs(workcor)
cor(workMadrid)
```


```{r,fig.height=4}
corrplot(cor(workcor), method="color", type="full", addCoef.col = "red", tl.col="black",number.cex = 0.75)
```


```{r}
workModelMadrid1 <- lm(energy_demand ~ E_1 + E_2 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + snow_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workMadrid)
```


```{r, fig.height=4}
vif(workModelMadrid1)
```


```{r}
workModelMadridFull <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + snow_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workMadrid)
vif(workModelMadridFull)
```


```{r, fig.height=5}
# to obtain residuals
res.fullmodel1 <- residuals(workModelMadridFull) 
# to obtain standardized residuals
std.res.fullmodel1 <- rstandard(workModelMadridFull) 
# to obtain fitted/predicted values
pred.fullmodel1 <- fitted.values(workModelMadridFull) 
par(mfrow=c(1,1))
qqnorm(y = std.res.fullmodel1, main = " Normal Q-Q Plot ",
       xlab = "Theoretical Quantiles", ylab = "Sample Quantiles")
qqline(y = std.res.fullmodel1)
#residual plot
resplotdata1 <- data.frame(std.res.fullmodel1, pred.fullmodel1)
resbf1 <- lm(std.res.fullmodel1 ~ pred.fullmodel1, data = resplotdata1)
plot(x = pred.fullmodel1, y = std.res.fullmodel1, ylab = "Standardized Residuals", xlab = "Predicted Values", main = "Residuals Plot", col = ifelse(std.res.fullmodel1 < -3,"red",ifelse(std.res.fullmodel1 > 3,"red","black")))
abline(h = 0, col="blue", lty=1)
abline(resbf1, col="red", lty=3)
abline(h = 3, col="green", lty=3)
abline(h=-3, col="green", lty=3)
legend("bottomleft", legend=c("Best fit line of standardized residuals", "Horizontal line y = 0.0", "Horizontal line, y = +/- 3"), fill = c("red","blue","green"), cex = 0.5)
```

```{r}
summary(workModelMadridFull)
df<- workModelMadridFull %>% 
  tidy()
#Get the variables which there p-value larger than 0.05 
df%>%
  filter(df$p.value>0.05)
workModelMadridPvalue <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workMadrid)
summary(workModelMadridPvalue)
```

+ p-value approach

```{r}
anova(workModelMadridPvalue, workModelMadridFull)
```

+ Best subset selection method

```{r, fig.height=5}
bestfits <- regsubsets(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + snow_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workMadrid, nbest = 1)
plot(bestfits, scale="adjr2")
```

```{r}
workModelMadridBestfit<-lm(energy_demand ~ E_1 + E_25 + temp + wind_speed + factor(day_night) + factor(time_band) + factor(season) , data = workMadrid)
summary(workModelMadridBestfit)
anova(workModelMadridBestfit, workModelMadridPvalue)
```

+ AIC with forward selection approach test method

```{r}
workNullModel<- lm(energy_demand ~ 1, data = workMadrid)
step.working <- stepAIC(workNullModel, scope = list(lower = workNullModel,
upper = workModelMadridFull), direction = "forward", trace=FALSE)
summary(step.working)
```

Same reduced model as P-value

+ Cross Validation test method

```{r,message=FALSE, warning=FALSE}
# Set seed for reproducibility in CV
set.seed(123)
# Set up repeated k-fold cross-validation, with k=10
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.cv.work <- train(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + snow_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workMadrid, method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:12),
trControl = train.control)
step.cv.work$results
summary(step.cv.work$finalModel)
```


```{r}
workModelMadridCross <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + wind_speed + rain_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workMadrid)
anova( workModelMadridCross, workModelMadridPvalue)
summary(workModelMadridCross)
```

+ OLS method

```{r fig.height=4}
#ModelOLS <- ols_step_all_possible(workModelMadridFull)
#cat("\n")
#plot(x = ModelOLS$n, y = ModelOLS$adjr)
#plot(x = ModelOLS$n, y = ModelOLS$aic)
#OLSsummary <- ModelOLS %>% group_by(n) %>%  filter(adjr == max(adjr))
#OLSsummary
```

Same reduced model as P-Value approach (thus commenting out given processing time)

## Select the final model

Model adjusted R-square from above methods (to the ten thousandths decimal place)

A) Full: 0.916

B) P-value: 0.916

C) Bestfits: 0.9154

D) AIC Method: 0.916

E) Cross-validation: 0.916

F) OLS-step: 0.916

**Final model is the workModelMadridPvalue as it has the highest Adjusted R-square and fewest predictors.**

```{r}
Method <- c('Full', 'P-value', 'BestFits', 'AIC Forward', 'Cross Validation')
Variables <- c(13,12,8,12,11)
Coefficients <- c(23,22,11,22,21)
Adjusted_R2 <- c(0.9160325,0.9160309,0.9153598,0.9160309,0.9159974)
MadridTable<-data.frame(Method,Variables,Coefficients,Adjusted_R2)
```

### Build the Final Model

```{r}
workFinal <- workModelMadridPvalue
```


### Check the linearity, heteroscedasticity, outlier and normal assumption

```{r, fig.height=4}
par(mfrow=c(2,2))
#Residual Plot
plot(x=fitted(workFinal), y=studres(workFinal), xlab = "Fitted Values",
ylab = "Studentized Residuals", main='Residuals Plot', col = ifelse(studres(workFinal) < -3,"red",ifelse(studres(workFinal) > 3,"red","black")))
abline(h=-3, col="red", lty=2)
abline(h=3, col="red", lty=2)
#norm test
std.res <- rstandard(workFinal)
ad.test(std.res)
qqnorm(studres(workFinal), pch = 1, frame = FALSE, main=expression("AD test =2.2*10"^-16))
qqline(studres(workFinal), col = "steelblue", lwd = 2)
#leverage 
lev<-hatvalues(workFinal)
cutlev = (2*length(coefficients(workFinal)))/nrow(workMadrid)
# Count and assess the number of leverage values > cut-off
potoutlier <- sum(lev > cutlev, na.rm=TRUE)
totcount = length(fitted(workFinal))
print(paste("The number of leverage points that are potential outliers is", potoutlier,".  This is", round(100*potoutlier/totcount,2),"% of the total number of predicted values, (", totcount,") thus not material."))
#barplot(lev, ylim = c(0, 2*cutlev))
plot(lev, type="h", ylim = c(0, 2*cutlev), ylab="Leverage for the working set")
abline(h = cutlev, col = "red", lty=2)
#cook distance
cookdist<-cooks.distance(workFinal)
#barplot(cookdist, ylim=c(0,1.01), main = "Cook's Distance plot")
#abline(h = 1, col = "red")
ggplot(as.data.frame(cookdist),aes(x=1:nrow(as.data.frame(cookdist)),y=cookdist)) +
  geom_line() + 
  geom_hline(aes(yintercept = 1,color = "red"))+
  scale_y_continuous(limits = c(0,1.1))+
  scale_y_break(breaks = c(0.01,0.9), scales =0.1) +
  guides(color = "none") +
  labs(title="Cook's Distance",
        x ="Index", y = "")
```

```{r}
# Opening the graphical device
# Customizing the output
pdf("MadridAssumptionCheck1.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   
# par(mfrow=c(2,2))
#residuals scatter plot
plot(x=fitted(workFinal), y=studres(workFinal), xlab = "Fitted Values",
ylab = "Studentized Residuals", main='Residuals Plot', col = ifelse(studres(workFinal) < -3,"red",ifelse(studres(workFinal) > 3,"red","black")))
abline(h=-3, col="red", lty=2)
abline(h=3, col="red", lty=2)
grid(10,10)
dev.off()


#normality plot
pdf("MadridAssumptionCheck2.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
qqnorm(studres(workFinal), pch = 1, frame = FALSE, main=expression("AD test =2.2*10"^-16))
qqline(studres(workFinal), col = "steelblue", lwd = 2)
grid(10,10)
dev.off()


#leverage plot
pdf("MadridAssumptionCheck3.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model

plot(lev, type="h", ylim = c(0, 2*cutlev), ylab="Leverage for Barcelona working set")
abline(h = cutlev, col = "red", lty=2)
grid(10,10)
dev.off()


#Cook's distance plot
pdf("MadridAssumptionCheck4.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
ggplot(as.data.frame(cookdist),aes(x=1:nrow(as.data.frame(cookdist)),y=cookdist)) +
  geom_line() + 
  geom_hline(aes(yintercept = 1,color = "red"))+
  scale_y_continuous(limits = c(0,1.1))+
  scale_y_break(breaks = c(0.005,0.9), scales =0.1) +
  guides(color = "none") +
  labs(title="Cook's Distance",
        x ="Index", y = "")
dev.off()

```

### Use the estimated model to predict the values in the evaluation set

```{r}
newdata <- evalMadrid[ c(-2,-14)]
predict.eval <- predict(workFinal, newdata)
plot(x = evalMadrid$energy_demand, y=predict.eval, xlab="Actual energy demand for the evaluation set",
ylab="Predicted energy demand for the evaluation set", main = "Madrid")
abline(a=0, b=1, col="blue")
grid(10,10)
```

Prepare report figure
```{r fig.height=4}
# Opening the graphical device
# Customizing the output
pdf("MadridEvalActComp.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   
par(mfrow=c(1,1))
plot(x = evalMadrid$energy_demand, y=predict.eval, xlab="Actual energy demand for the evaluation set",
ylab="Predicted energy demand for the evaluation set", main = "Madrid", cex.lab = 0.75)
abline(a=0, b=1, col="blue")
grid(10,10)

# Closing the graphical device
dev.off()
```

### Compute the RMSE for the predictions

```{r}
rootmse <- rmse(evalMadrid$energy_demand, predict.eval)
rootmse <- rmse(evaldata$energy_demand, predict.eval)
print(paste("The root mean square error of the predicted vs actual",nrow(evaldata),"hourly energy_data points for Madrid over the 2015-2018 timespan is an energy demand difference of",round(rootmse,2),"MWh."))
cat("\n")
demandcompdf <- data.frame("Actual (MWh)" = evalMadrid$energy_demand,"Predicted (MWh)" = predict.eval)
head(demandcompdf,4L)
cat("\n")
print(paste("For example, the first actual demand value is",round(evaldata$energy_demand[1],2),"MWh and the corresponding predicted value is",round(predict.eval[1],2),"MWh. The percent error between this predicted value relative to the actual value is",round((100*(predict.eval[1] - evaldata$energy_demand[1])/evaldata$energy_demand[1]),2),"%."))
cat("\n")
diffpct <- c((abs(evaldata$energy_demand - predict.eval))/evaldata$energy_demand)
print(paste("The mean difference between the",nrow(evaldata),"evaluation data set actual and model predicted values is",round(100*mean(diffpct),2),"%."))
```

## Compute and analyze Hat Matrix for Evaluation data set
Obtain X matrix from the model
```{r}
Predictors <-rownames(summary(workFinal)$coefficients[,])
workpredictors <- as.data.frame(Predictors)
X <- model.matrix(workFinal)
class(X)
cat("\n")
dim(X)
cat("\n")
head(X, 4L)
```

Create x_new matrix using the validation data.  Note that the purpose of running the evaluation regression here is to extract all of the correct predictor coefficients.
```{r}
evalFinal <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + 
    rain_duration + factor(day_night) + factor(time_band) + factor(season) + 
    factor(weather_main), data = evalMadrid)
Predictors <-rownames(summary(evalFinal)$coefficients[,])
evalpredictors <- as.data.frame(Predictors)
x_new1 <- model.matrix(evalFinal)
cat("\n")
dim(x_new1)
cat("\n")
#Check if there's a predictor difference between the work and eval matrices
workevalcompare <-anti_join(workpredictors,evalpredictors)
cat("\n")
#If workevalcompare is 'No data available in table', run the next line of code and skip to line 816
x_new <- x_new1
#Otherwise when workevalcompare is not 'No data available in table', run the following lines of code after identifying which column is missing between X and x_new1 (no discrepancy was found between workMadrid but not in evalMadrid).
#x_new_ <-as.data.frame(cbind(x_new1,rep(0,nrow(x_new1))))
#dummy_xnew<- x_new_ %>% relocate(V21,.before = 16) %>% rename("factor(weather_main)4" = V21)
#x_new <- as.matrix(dummy_xnew)  
cat("\n")
class(x_new)
cat("\n")
dim(x_new)
cat("\n")
head(x_new, 4L)
```

Compute the hat values for the validation data
```{r}
h_new_mid <- solve(t(X)%*%X)
dim(h_new_mid)
h_new <- x_new%*%h_new_mid%*%t(x_new)
dim(h_new)
```

Plot the hatvalues for the data in the evaluation set
```{r fig.height=4}
#cutlev2 = (2*length(coefficients(evalFinal)))/nrow(evalMadrid)
# Count and assess the number of leverage values > cut-off
potoutlier2 <- sum(diag(h_new) > cutlev, na.rm=TRUE)
totcount2 = nrow(evalMadrid)
plot(diag(h_new), type = "h", ylab = "Leverage for the validation data set", ylim = c(0,2*cutlev), main = "Madrid")
abline(h=cutlev, col="red", lty=2)
print(paste("The number of leverage points that are potential outliers is", potoutlier2,".  This is", round(100*potoutlier2/totcount2,2),"% of the total number of predicted values, (", totcount2,") thus not material."))
```

Side-by-side comparision
```{r fig.height=4}
par(mfrow=c(1,2))
plot(lev, type="h", ylim = c(0, 2*cutlev), ylab="Leverage for the working data set", main = "Madrid")
abline(h = cutlev, col = "red", lty=2)
plot(diag(h_new), type = "h", ylim = c(0,2*cutlev), ylab = "Leverage for the evaluation data set", main = "Madrid")
abline(h=cutlev, col="red", lty=2)
print(paste("The leverage cutoff exceedences in the evaluation set is",round(100*potoutlier2/totcount2,2),"% of the total number of observed values (", totcount2,") and is comparable to that of the working set,", round(100*potoutlier/totcount,2),"% of the total number of predicted values (", totcount,")."))
```

Prepare report figure
```{r}
# Opening the graphical device
# Customizing the output
pdf("MadridLevComp.pdf",         # File name
    width = 14, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   
par(mfrow=c(1,2))
plot(lev, type="h", ylab = "Leverage", ylim = c(0,2*cutlev), yaxt = 'n', main = "Madrid Working Data", res =600)
abline(h = cutlev, col = "red", lty=2)
axis(side = 2, at=seq(0,2*round(cutlev,5),by=round(cutlev,5)))

plot(diag(h_new), type = "h", ylab = "", ylim = c(0,2*cutlev), yaxt = 'n', main = "Madrid Evaluation Data")
abline(h=cutlev, col="red", lty=2)
axis(side = 2, at=seq(0,2*round(cutlev,5),by=round(cutlev,5)))

# Closing the graphical device
dev.off()
```

## Seville Model

```{r}
evaldata<-read.csv('data/evaluation_Seville.csv')
workdata<-read.csv('data/working_Seville.csv')
#remove index, time_ID and snow_duration columns from "eval" analysis dataframe
evalSeville<-evaldata[c(-1,-2,-11)] 
#remove index, time_ID and snow_duration columns from "work" analysis dataframe
workSeville<-workdata[c(-1,-2,-11)] 
head(workSeville, 4L)
```


```{r,,fig.height=5}
# Correlation matrix for numeric predictors only (categorical predictors excluded)
workcor<-workSeville[c(-9:-12) ]
ggpairs(workcor)
cor(workSeville)
```


```{r,fig.height=4}
corrplot(cor(workcor), method="color", type="full", addCoef.col = "red", tl.col="black",number.cex = 0.75)
```


```{r}
workModelSeville1 <- lm(energy_demand ~ E_1 + E_2 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workSeville)
```


```{r, fig.height=4}
vif(workModelSeville1)
```


```{r}
workModelSevilleFull <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workSeville)
vif(workModelSevilleFull)
```


```{r, fig.height=5}
# to obtain residuals
res.fullmodel1 <- residuals(workModelSevilleFull) 
# to obtain standardized residuals
std.res.fullmodel1 <- rstandard(workModelSevilleFull) 
# to obtain fitted/predicted values
pred.fullmodel1 <- fitted.values(workModelSevilleFull) 
par(mfrow=c(1,1))
qqnorm(y = std.res.fullmodel1, main = " Normal Q-Q Plot ",
       xlab = "Theoretical Quantiles", ylab = "Sample Quantiles")
qqline(y = std.res.fullmodel1)
#residual plot
resplotdata1 <- data.frame(std.res.fullmodel1, pred.fullmodel1)
resbf1 <- lm(std.res.fullmodel1 ~ pred.fullmodel1, data = resplotdata1)
plot(x = pred.fullmodel1, y = std.res.fullmodel1, ylab = "Standardized Residuals", xlab = "Predicted Values", main = "Residuals Plot", col = ifelse(std.res.fullmodel1 < -3,"red",ifelse(std.res.fullmodel1 > 3,"red","black")))
abline(h = 0, col="blue", lty=1)
abline(resbf1, col="red", lty=3)
abline(h = 3, col="green", lty=3)
abline(h=-3, col="green", lty=3)
legend("bottomleft", legend=c("Best fit line of standardized residuals", "Horizontal line y = 0.0", "Horizontal line, y = +/- 3"), fill = c("red","blue","green"), cex = 0.5)
```

```{r}
summary(workModelSevilleFull)
df<- workModelSevilleFull %>% 
  tidy()
#Get the variables which there p-value larger than 0.05 
df%>%
  filter(df$p.value>0.05)
workModelSevillePvalue <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workSeville)
summary(workModelSevillePvalue)
```

+ p-value approach

```{r}
anova(workModelSevillePvalue, workModelSevilleFull)
```

+ Best subset selection method

```{r, fig.height=5}
bestfits <- regsubsets(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workSeville, nbest = 1)
plot(bestfits, scale="adjr2")
```

```{r}
workModelSevilleBestfit<-lm(energy_demand ~ E_1 + E_25 + temp + humidity + wind_speed + factor(day_night) + factor(season) , data = workSeville)
summary(workModelSevilleBestfit)
anova(workModelSevilleBestfit, workModelSevillePvalue)
```

+ AIC with forward selection approach test method

```{r}
workNullModel<- lm(energy_demand ~ 1, data = workSeville)
step.working <- stepAIC(workNullModel, scope = list(lower = workNullModel,
upper = workModelSevilleFull), direction = "forward", trace=FALSE)
summary(step.working)
```

+ Cross Validation test method

```{r,warning=FALSE}
# Set seed for reproducibility in CV
set.seed(123)
# Set up repeated k-fold cross-validation, with k=10
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.cv.work <- train(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workSeville, method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:11),
trControl = train.control)
step.cv.work$results
summary(step.cv.work$finalModel)
```

Reduced model same as P-value approach

+ OLS method

```{r fig.height=4}
#ModelOLS <- ols_step_all_possible(workModelSevilleFull)
#cat("\n")
#plot(x = ModelOLS$n, y = ModelOLS$adjr)
#plot(x = ModelOLS$n, y = ModelOLS$aic)
#OLSsummary <- ModelOLS %>% group_by(n) %>%  filter(adjr == max(adjr))
```

Same reduced model as P-Value approach (thus commenting out given processing time)

## Select the final model

Model adjusted R-square from above methods (to the ten thousandths decimal place)

A) Full: 0.9199

B) P-value: 0.9199

C) Bestfits: 0.9189

D) AIC Method: 0.9199

E) Cross-validation: 0.9199

**Final model is the workModelSevillePvalue as it has the highest Adjusted R-square and fewest predictors.**

```{r}
Method <- c('Full','P-value', 'BestFits', 'AIC Forward', 'Cross Validation')
Variables <- c(12,11,8,12,11)
Coefficients <- c(24,23,10,24,23)
Adjusted_R2 <- c(0.9199264,0.9199208,0.9189457,0.9199264,0.9199208)
SevilleTable<-data.frame(Method,Variables,Coefficients,Adjusted_R2)
```

### Build the Final Model

```{r}
workFinal <- workModelSevillePvalue
```


### Check the linearity, heteroscedasticity, outlier and normal assumption

```{r, fig.height=4}
par(mfrow=c(2,2))
#Residual Plot
plot(x=fitted(workFinal), y=studres(workFinal), xlab = "Fitted Values",
ylab = "Studentized Residuals", main='Residuals Plot', col = ifelse(studres(workFinal) < -3,"red",ifelse(studres(workFinal) > 3,"red","black")))
abline(h=-3, col="red", lty=2)
abline(h=3, col="red", lty=2)
#norm test
std.res <- rstandard(workFinal)
ad.test(std.res)
qqnorm(studres(workFinal), pch = 1, frame = FALSE, main=expression("AD test =2.2*10"^-16))
qqline(studres(workFinal), col = "steelblue", lwd = 2)
#leverage 
lev<-hatvalues(workFinal)
cutlev = (2*length(coefficients(workFinal)))/nrow(workSeville)
# Count and assess the number of leverage values > cut-off
potoutlier <- sum(lev > cutlev, na.rm=TRUE)
totcount = length(fitted(workFinal))
print(paste("The number of leverage points that are potential outliers is", potoutlier,".  This is", round(100*potoutlier/totcount,2),"% of the total number of predicted values, (", totcount,") thus not material."))
#barplot(lev, ylim = c(0, 2*cutlev))
plot(lev, type="h", ylim = c(0, 2*cutlev), ylab="Leverage for the working set")
abline(h = cutlev, col = "red", lty=2)
#cook distance
cookdist<-cooks.distance(workFinal)
#barplot(cookdist, ylim=c(0,1.01), main = "Cook's Distance plot")
#abline(h = 1, col = "red")
ggplot(as.data.frame(cookdist),aes(x=1:nrow(as.data.frame(cookdist)),y=cookdist)) +
  geom_line() + 
  geom_hline(aes(yintercept = 1,color = "red"))+
  scale_y_continuous(limits = c(0,1.1))+
  scale_y_break(breaks = c(0.03,0.9), scales =0.1) +
  guides(color = "none") +
  labs(title="Cook's Distance",
        x ="Index", y = "")
```

```{r}
# Opening the graphical device
# Customizing the output
pdf("SevilleAssumptionCheck1.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   
# par(mfrow=c(2,2))
#residuals scatter plot
plot(x=fitted(workFinal), y=studres(workFinal), xlab = "Fitted Values",
ylab = "Studentized Residuals", main='Residuals Plot', col = ifelse(studres(workFinal) < -3,"red",ifelse(studres(workFinal) > 3,"red","black")))
abline(h=-3, col="red", lty=2)
abline(h=3, col="red", lty=2)
grid(10,10)
dev.off()


#normality plot
pdf("SevilleAssumptionCheck2.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
qqnorm(studres(workFinal), pch = 1, frame = FALSE, main=expression("AD test =2.2*10"^-16))
qqline(studres(workFinal), col = "steelblue", lwd = 2)
grid(10,10)
dev.off()


#leverage plot
pdf("SevilleAssumptionCheck3.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model

plot(lev, type="h", ylim = c(0, 2*cutlev), ylab="Leverage for Barcelona working set")
abline(h = cutlev, col = "red", lty=2)
grid(10,10)
dev.off()


#Cook's distance plot
pdf("SevilleAssumptionCheck4.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
ggplot(as.data.frame(cookdist),aes(x=1:nrow(as.data.frame(cookdist)),y=cookdist)) +
  geom_line() + 
  geom_hline(aes(yintercept = 1,color = "red"))+
  scale_y_continuous(limits = c(0,1.1))+
  scale_y_break(breaks = c(0.005,0.9), scales =0.1) +
  guides(color = "none") +
  labs(title="Cook's Distance",
        x ="Index", y = "")
dev.off()

```

### Use the estimated model to predict the values in the evaluation set

```{r}
newdata <- evalSeville[ c(-2,-13)]
predict.eval <- predict(workFinal, newdata)
plot(x = evalSeville$energy_demand, y=predict.eval, xlab="Actual energy demand for the evaluation set",
ylab="Predicted energy demand for the evaluation set", main = "Seville")
abline(a=0, b=1, col="blue")
grid(10,10)
```

Prepare report figure
```{r fig.height=4}
# Opening the graphical device
# Customizing the output
pdf("SevilleEvalActComp.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   
par(mfrow=c(1,1))
plot(x = evalSeville$energy_demand, y=predict.eval, xlab="Actual energy demand for the evaluation set",
ylab="Predicted energy demand for the evaluation set", main = "Seville", cex.lab = 0.75)
abline(a=0, b=1, col="blue")
grid(10,10)

# Closing the graphical device
dev.off()
```

### Compute the RMSE for the predictions

```{r}
rootmse <- rmse(evalSeville$energy_demand, predict.eval)
rootmse <- rmse(evaldata$energy_demand, predict.eval)
print(paste("The root mean square error of the predicted vs actual",nrow(evaldata),"hourly energy_data points for Seville over the 2015-2018 timespan is an energy demand difference of",round(rootmse,2),"MWh."))
cat("\n")
demandcompdf <- data.frame("Actual (MWh)" = evalSeville$energy_demand,"Predicted (MWh)" = predict.eval)
head(demandcompdf,4L)
cat("\n")
print(paste("For example, the first actual demand value is",round(evaldata$energy_demand[1],2),"MWh and the corresponding predicted value is",round(predict.eval[1],2),"MWh. The percent error between this predicted value relative to the actual value is",round((100*(predict.eval[1] - evaldata$energy_demand[1])/evaldata$energy_demand[1]),2),"%."))
cat("\n")
diffpct <- c((abs(evaldata$energy_demand - predict.eval))/evaldata$energy_demand)
print(paste("The mean difference between the",nrow(evaldata),"evaluation data set actual and model predicted values is",round(100*mean(diffpct),2),"%."))
```

## Compute and analyze Hat Matrix for Evaluation data set
Obtain X matrix from the model
```{r}
Predictors <-rownames(summary(workFinal)$coefficients[,])
workpredictors <- as.data.frame(Predictors)
X <- model.matrix(workFinal)
class(X)
cat("\n")
dim(X)
cat("\n")
head(X, 4L)
```

Create x_new matrix using the validation data.  Note that the purpose of running the evaluation regression here is to extract all of the correct predictor coefficients.
```{r}
evalFinal <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + 
                  wind_speed + factor(day_night) + factor(time_band) + 
                  factor(season) + factor(weather_main), data = evalSeville)
Predictors <-rownames(summary(evalFinal)$coefficients[,])
evalpredictors <- as.data.frame(Predictors)
x_new1 <- model.matrix(evalFinal)
cat("\n")
dim(x_new1)
cat("\n")
#Check if there's a predictor difference between the work and eval matrices
workevalcompare <-anti_join(workpredictors,evalpredictors)
cat("\n")
#If workevalcompare is 'No data available in table', run the next line of code and skip to line 1580
#x_new <- x_new1
#Otherwise when workevalcompare is not 'No data available in table', run the following lines of code after identifying which column is missing between X and x_new1 (for Seville, it was found that "factor(weather_main)11" appears in workSeville but not in evalSeville).
x_new_ <-as.data.frame(cbind(x_new1,rep(0,nrow(x_new1))))
dummy_xnew<- x_new_ %>% relocate(V23,.before = 22) %>% rename("factor(weather_main)11" = V23)
x_new <- as.matrix(dummy_xnew)  
cat("\n")
class(x_new)
cat("\n")
dim(x_new)
cat("\n")
head(x_new, 4L)
```

Compute the hat values for the validation data
```{r}
h_new_mid <- solve(t(X)%*%X)
dim(h_new_mid)
h_new <- x_new%*%h_new_mid%*%t(x_new)
dim(h_new)
```

Plot the hatvalues for the data in the evaluation set
```{r fig.height=4}
#cutlev2 = (2*length(coefficients(evalFinal)))/nrow(evalSeville)
# Count and assess the number of leverage values > cut-off
potoutlier2 <- sum(diag(h_new) > cutlev, na.rm=TRUE)
totcount2 = nrow(evalSeville)
plot(diag(h_new), type = "h", ylab = "Leverage for the validation data set", ylim = c(0,2*cutlev), main = "Seville")
abline(h=cutlev, col="red", lty=2)
print(paste("The number of leverage points that are potential outliers is", potoutlier2,".  This is", round(100*potoutlier2/totcount2,2),"% of the total number of predicted values, (", totcount2,") thus not material."))
```

Side-by-side comparision
```{r fig.height=4}
par(mfrow=c(1,2))
plot(lev, type="h", ylim = c(0, 2*cutlev), ylab="Leverage for the working data set", main = "Seville")
abline(h = cutlev, col = "red", lty=2)
plot(diag(h_new), type = "h", ylim = c(0,2*cutlev), ylab = "Leverage for the evaluation data set", main = "Seville")
abline(h=cutlev, col="red", lty=2)
print(paste("The leverage cutoff exceedences in the evaluation set is",round(100*potoutlier2/totcount2,2),"% of the total number of observed values (", totcount2,") and is comparable to that of the working set,", round(100*potoutlier/totcount,2),"% of the total number of predicted values (", totcount,")."))
```

Prepare report figure
```{r}
# Opening the graphical device
# Customizing the output
pdf("SevilleLevComp.pdf",         # File name
    width = 14, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   
par(mfrow=c(1,2))
plot(lev, type="h", ylab = "Leverage", ylim = c(0,2*cutlev), yaxt = 'n', main = "Seville Working Data", res =600)
abline(h = cutlev, col = "red", lty=2)
axis(side = 2, at=seq(0,2*round(cutlev,5),by=round(cutlev,5)))

plot(diag(h_new), type = "h", ylab = "", ylim = c(0,2*cutlev), yaxt = 'n', main = "Seville Evaluation Data")
abline(h=cutlev, col="red", lty=2)
axis(side = 2, at=seq(0,2*round(cutlev,5),by=round(cutlev,5)))

# Closing the graphical device
dev.off()
```

## Valencia Model

```{r}
evaldata<-read.csv('data/evaluation_Valencia.csv')
workdata<-read.csv('data/working_Valencia.csv')
#remove index and time_ID columns from "eval" analysis dataframe
evalValencia<-evaldata[c(-1,-2)] 
#remove index and time_ID columns from "work" analysis dataframe
workValencia<-workdata[c(-1,-2)] 
head(workValencia, 4L)
```


```{r,,fig.height=5}
# Correlation matrix for numeric predictors only (categorical predictors excluded)
workcor<-workValencia[c(-10:-13) ]
ggpairs(workcor)
cor(workValencia)
```


```{r,fig.height=4}
corrplot(cor(workcor), method="color", type="full", addCoef.col = "red", tl.col="black",number.cex = 0.75)
```


```{r}
workModelValencia1 <- lm(energy_demand ~ E_1 + E_2 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + snow_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workValencia)
```


```{r, fig.height=4}
vif(workModelValencia1)
```


```{r}
workModelValenciaFull <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + snow_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workValencia)
vif(workModelValenciaFull)
```


```{r, fig.height=5}
# to obtain residuals
res.fullmodel1 <- residuals(workModelValenciaFull) 
# to obtain standardized residuals
std.res.fullmodel1 <- rstandard(workModelValenciaFull) 
# to obtain fitted/predicted values
pred.fullmodel1 <- fitted.values(workModelValenciaFull) 
par(mfrow=c(1,1))
qqnorm(y = std.res.fullmodel1, main = " Normal Q-Q Plot ",
       xlab = "Theoretical Quantiles", ylab = "Sample Quantiles")
qqline(y = std.res.fullmodel1)
#residual plot
resplotdata1 <- data.frame(std.res.fullmodel1, pred.fullmodel1)
resbf1 <- lm(std.res.fullmodel1 ~ pred.fullmodel1, data = resplotdata1)
plot(x = pred.fullmodel1, y = std.res.fullmodel1, ylab = "Standardized Residuals", xlab = "Predicted Values", main = "Residuals Plot", col = ifelse(std.res.fullmodel1 < -3,"red",ifelse(std.res.fullmodel1 > 3,"red","black")))
abline(h = 0, col="blue", lty=1)
abline(resbf1, col="red", lty=3)
abline(h = 3, col="green", lty=3)
abline(h=-3, col="green", lty=3)
legend("bottomleft", legend=c("Best fit line of standardized residuals", "Horizontal line y = 0.0", "Horizontal line, y = +/- 3"), fill = c("red","blue","green"), cex = 0.5)
```

```{r}
summary(workModelValenciaFull)
df<- workModelValenciaFull %>% 
  tidy()
#Get the variables which there p-value larger than 0.05 
df%>%
  filter(df$p.value>0.05)
workModelValenciaPvalue <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workValencia)
summary(workModelValenciaPvalue)
```

+ p-value approach

```{r}
anova(workModelValenciaPvalue, workModelValenciaFull)
```

+ Best subset selection method

```{r, fig.height=5}
bestfits <- regsubsets(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + snow_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workValencia, nbest = 1)
plot(bestfits, scale="adjr2")
```

```{r}
workModelValenciaBestfit<-lm(energy_demand ~ E_1 + E_25 + temp + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workValencia)
summary(workModelValenciaBestfit)
anova(workModelValenciaBestfit, workModelValenciaPvalue)
```

+ AIC with forward selection approach test method

```{r}
workNullModel<- lm(energy_demand ~ 1, data = workValencia)
step.working <- stepAIC(workNullModel, scope = list(lower = workNullModel,
upper = workModelValenciaFull), direction = "forward", trace=FALSE)
summary(step.working)
```

+ Cross Validation test method

```{r}
# Set seed for reproducibility in CV
set.seed(123)
# Set up repeated k-fold cross-validation, with k=10
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.cv.work <- train(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + snow_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workValencia, method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:12),
trControl = train.control)
step.cv.work$results
summary(step.cv.work$finalModel)
```


```{r}
workModelValenciaCross <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = workValencia)
anova( workModelValenciaCross, workModelValenciaPvalue)
summary(workModelValenciaCross)
```

+ OLS method

```{r fig.height=4}
#ModelOLS <- ols_step_all_possible(workModelValenciaFull)
#cat("\n")
#plot(x = ModelOLS$n, y = ModelOLS$adjr)
#plot(x = ModelOLS$n, y = ModelOLS$aic)
#OLSsummary <- ModelOLS %>% group_by(n) %>%  filter(adjr == max(adjr))
```

Same reduced model as P-Value approach (thus commenting out given processing time)

## Select the final model

Model adjusted R-square from above methods (to the ten thousandths decimal place)

A) Full: 0.9177

B) P-value: 0.9177

C) Bestfits: 0.9174

D) AIC Method: 0.9177

E) Cross-validation: 0.9176

**Final model is the workModelValenciaPvalue as it has the highest Adjusted R-square and fewest predictors.**

```{r}
Method <- c('Full','P-value', 'BestFits', 'AIC Forward', 'Corss Validation')
Variables <- c(13,12,8,12,9)
Coefficients <- c(23,22,18,22,19)
Adjusted_R <- c(0.9176974,0.9176977,0.9173514,0.9176977,0.9175922)
ValenciaTable<-data.frame(Method,Variables,Coefficients,Adjusted_R)
```

### Build the Final Model

```{r}
workFinal <- workModelValenciaPvalue
```


### Check the linearity, heteroscedasticity, outlier and normal assumption

```{r, fig.height=4}
par(mfrow=c(2,2))
#Residual Plot
plot(x=fitted(workFinal), y=studres(workFinal), xlab = "Fitted Values",
ylab = "Studentized Residuals", main='Residuals Plot', col = ifelse(studres(workFinal) < -3,"red",ifelse(studres(workFinal) > 3,"red","black")))
abline(h=-3, col="red", lty=2)
abline(h=3, col="red", lty=2)
#norm test
std.res <- rstandard(workFinal)
ad.test(std.res)
qqnorm(studres(workFinal), pch = 1, frame = FALSE, main=expression("AD test =2.2*10"^-16))
qqline(studres(workFinal), col = "steelblue", lwd = 2)
#leverage 
lev<-hatvalues(workFinal)
cutlev = (2*length(coefficients(workFinal)))/nrow(workValencia)
# Count and assess the number of leverage values > cut-off
potoutlier <- sum(lev > cutlev, na.rm=TRUE)
totcount = length(fitted(workFinal))
print(paste("The number of leverage points that are potential outliers is", potoutlier,".  This is", round(100*potoutlier/totcount,2),"% of the total number of predicted values, (", totcount,") thus not material."))
#barplot(lev, ylim = c(0, 2*cutlev))
plot(lev, type="h", ylim = c(0, 2*cutlev), ylab="Leverage for the working set")
abline(h = cutlev, col = "red", lty=2)
#cook distance
cookdist<-cooks.distance(workFinal)
#barplot(cookdist, ylim=c(0,1.01), main = "Cook's Distance plot")
#abline(h = 1, col = "red")
ggplot(as.data.frame(cookdist),aes(x=1:nrow(as.data.frame(cookdist)),y=cookdist)) +
  geom_line() + 
  geom_hline(aes(yintercept = 1,color = "red"))+
  scale_y_continuous(limits = c(0,1.1))+
  scale_y_break(breaks = c(0.05,0.9), scales =0.1) +
  guides(color = "none") +
  labs(title="Cook's Distance",
        x ="Index", y = "")
```


```{r}
# Opening the graphical device
# Customizing the output
pdf("ValenciaAssumptionCheck1.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   
# par(mfrow=c(2,2))
#residuals scatter plot
plot(x=fitted(workFinal), y=studres(workFinal), xlab = "Fitted Values",
ylab = "Studentized Residuals", main='Residuals Plot', col = ifelse(studres(workFinal) < -3,"red",ifelse(studres(workFinal) > 3,"red","black")))
abline(h=-3, col="red", lty=2)
abline(h=3, col="red", lty=2)
grid(10,10)
dev.off()


#normality plot
pdf("ValenciaAssumptionCheck2.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
qqnorm(studres(workFinal), pch = 1, frame = FALSE, main=expression("AD test =2.2*10"^-16))
qqline(studres(workFinal), col = "steelblue", lwd = 2)
grid(10,10)
dev.off()


#leverage plot
pdf("ValenciaAssumptionCheck3.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model

plot(lev, type="h", ylim = c(0, 2*cutlev), ylab="Leverage for Barcelona working set")
abline(h = cutlev, col = "red", lty=2)
grid(10,10)
dev.off()


#Cook's distance plot
pdf("ValenciaAssumptionCheck4.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
ggplot(as.data.frame(cookdist),aes(x=1:nrow(as.data.frame(cookdist)),y=cookdist)) +
  geom_line() + 
  geom_hline(aes(yintercept = 1,color = "red"))+
  scale_y_continuous(limits = c(0,1.1))+
  scale_y_break(breaks = c(0.005,0.9), scales =0.1) +
  guides(color = "none") +
  labs(title="Cook's Distance",
        x ="Index", y = "")
dev.off()

```

### Use the estimated model to predict the values in the evaluation set

```{r}
newdata <- evalValencia[ c(-2,-14)]
predict.eval <- predict(workFinal, newdata)
plot(x = evalValencia$energy_demand, y=predict.eval, xlab="actual energy demand for the evaluation set",
ylab="predicted energy demand for the evaluation set", main = "Valencia")
abline(a=0, b=1, col="blue")
grid(10,10)
```

Prepare report figure
```{r fig.height=4}
# Opening the graphical device
# Customizing the output
pdf("ValenciaEvalActComp.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   
par(mfrow=c(1,1))
plot(x = evalValencia$energy_demand, y=predict.eval, xlab="Actual energy demand for the evaluation set",
ylab="Predicted energy demand for the evaluation set", main = "Valencia", cex.lab = 0.75)
abline(a=0, b=1, col="blue")
grid(10,10)

# Closing the graphical device
dev.off()
```

### Compute the RMSE for the predictions

```{r}
rootmse <- rmse(evalValencia$energy_demand, predict.eval)
rootmse <- rmse(evaldata$energy_demand, predict.eval)
print(paste("The root mean square error of the predicted vs actual",nrow(evaldata),"hourly energy_data points for Valencia over the 2015-2018 timespan is an energy demand difference of",round(rootmse,2),"MWh."))
cat("\n")
demandcompdf <- data.frame("Actual (MWh)" = evalValencia$energy_demand,"Predicted (MWh)" = predict.eval)
head(demandcompdf,4L)
cat("\n")
print(paste("For example, the first actual demand value is",round(evaldata$energy_demand[1],2),"MWh and the corresponding predicted value is",round(predict.eval[1],2),"MWh. The percent error between this predicted value relative to the actual value is",round((100*(predict.eval[1] - evaldata$energy_demand[1])/evaldata$energy_demand[1]),2),"%."))
cat("\n")
diffpct <- c((abs(evaldata$energy_demand - predict.eval))/evaldata$energy_demand)
print(paste("The mean difference between the",nrow(evaldata),"evaluation data set actual and model predicted values is",round(100*mean(diffpct),2),"%."))
```

## Compute and analyze Hat Matrix for Evaluation data set
Obtain X matrix from the model
```{r}
Predictors <-rownames(summary(workFinal)$coefficients[,])
workpredictors <- as.data.frame(Predictors)
X <- model.matrix(workFinal)
class(X)
cat("\n")
dim(X)
cat("\n")
head(X, 4L)
```

Create x_new matrix using the validation data.  Note that the purpose of running the evaluation regression here is to extract all of the correct predictor coefficients.
```{r}
evalFinal <- lm(energy_demand ~ E_1 + E_25 + temp + humidity + pressure + wind_speed + rain_duration + factor(day_night) + factor(time_band) + factor(season) + factor(weather_main), data = evalValencia)
Predictors <-rownames(summary(evalFinal)$coefficients[,])
evalpredictors <- as.data.frame(Predictors)
x_new1 <- model.matrix(evalFinal)
cat("\n")
dim(x_new1)
cat("\n")
#Check if there's a predictor difference between the work and eval matrices
workevalcompare <-anti_join(workpredictors,evalpredictors)
cat("\n")
#If workevalcompare is 'No data available in table', run the next line of code and skip to line 1960
x_new <- x_new1
#Otherwise when workevalcompare is not 'No data available in table', run the following lines of code after identifying which column is missing between X and x_new1 (no discrepancy was found between workValencia but not in evalValencia).
#x_new_ <-as.data.frame(cbind(x_new1,rep(0,nrow(x_new1))))
#dummy_xnew<- x_new_ %>% relocate(V21,.before = 16) %>% rename("factor(weather_main)4" = V21)
#x_new <- as.matrix(dummy_xnew)  
cat("\n")
class(x_new)
cat("\n")
dim(x_new)
cat("\n")
head(x_new, 4L)
```

Compute the hat values for the validation data
```{r}
h_new_mid <- solve(t(X)%*%X)
dim(h_new_mid)
h_new <- x_new%*%h_new_mid%*%t(x_new)
dim(h_new)
```

Plot the hatvalues for the data in the evaluation set
```{r fig.height=4}
#cutlev2 = (2*length(coefficients(evalFinal)))/nrow(evalValencia)
# Count and assess the number of leverage values > cut-off
potoutlier2 <- sum(diag(h_new) > cutlev, na.rm=TRUE)
totcount2 = nrow(evalValencia)
plot(diag(h_new), type = "h", ylab = "Leverage for the validation data set", ylim = c(0,2*cutlev), main = "Valencia")
abline(h=cutlev, col="red", lty=2)
print(paste("The number of leverage points that are potential outliers is", potoutlier2,".  This is", round(100*potoutlier2/totcount2,2),"% of the total number of predicted values, (", totcount2,") thus not material."))
```

Side-by-side comparision
```{r fig.height=4}
par(mfrow=c(1,2))
plot(lev, type="h", ylim = c(0, 2*cutlev), ylab="Leverage for the working data set", main = "Valencia")
abline(h = cutlev, col = "red", lty=2)
plot(diag(h_new), type = "h", ylim = c(0,2*cutlev), ylab = "Leverage for the evaluation data set", main = "Valencia")
abline(h=cutlev, col="red", lty=2)
print(paste("The leverage cutoff exceedences in the evaluation set is",round(100*potoutlier2/totcount2,2),"% of the total number of observed values (", totcount2,") and is comparable to that of the working set,", round(100*potoutlier/totcount,2),"% of the total number of predicted values (", totcount,")."))
```

Prepare report figure
```{r}
# Opening the graphical device
# Customizing the output
pdf("ValenciaLevComp.pdf",         # File name
    width = 14, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   
par(mfrow=c(1,2))
plot(lev, type="h", ylab = "Leverage", ylim = c(0,2*cutlev), yaxt = 'n', main = "Valencia Working Data", res =600)
abline(h = cutlev, col = "red", lty=2)
axis(side = 2, at=seq(0,2*round(cutlev,5),by=round(cutlev,5)))

plot(diag(h_new), type = "h", ylab = "", ylim = c(0,2*cutlev), yaxt = 'n', main = "Valencia Evaluation Data")
abline(h=cutlev, col="red", lty=2)
axis(side = 2, at=seq(0,2*round(cutlev,5),by=round(cutlev,5)))

# Closing the graphical device
dev.off()
```

# Tree Based Modelling 

## Code for Barcelona

### organize data


```{r}
# import data raw data set
w_BCN_raw <- read.csv("data/working_Barcelona.csv")
e_BCN_raw <- read.csv("data/evaluation_Barcelona.csv")
w_BCN <- w_BCN_raw[,-c(1,2)]
e_BCN <- e_BCN_raw[,-c(1,2)]
```

```{r}
# changes of data type
class(w_BCN$day_night) = "category"
class(w_BCN$time_band) = "category"
class(w_BCN$season) = "category"
class(w_BCN$weather_main) = "category"

class(e_BCN$day_night) = "category"
class(e_BCN$time_band) = "category"
class(e_BCN$season) = "category"
class(e_BCN$weather_main) = "category"

# check data types
str(w_BCN)
str(e_BCN) 
summary(w_BCN)

```

### RF modeling

```{r}
# fit random forest model
# default RF model
set.seed(1234)
rf <- randomForest(formula = energy_demand ~ ., data = w_BCN)

# find the num of trees in the forest with smallest MSE
plot(rf)
grid(10,10)
summary(rf)


```




```{r}

testBCN <- rf$mse
testBCN <- as.data.frame(testBCN)
threshold = 0.05 # Threshold Error Margin (5 %) 

#Visually, the slope become flat around 100 in the plot; determine 95% threshold value
InflectionValue <- testBCN[nrow(testBCN),] + threshold*testBCN[nrow(testBCN),]

InflectionIndex <- length(testBCN[testBCN>InflectionValue]) + 1
InflectionIndex_BCN <- InflectionIndex
print(paste("95% of the of the error has been reduced with",InflectionIndex_BCN,"trees."))

```



```{r fig.height=4}
# modify the model with best mtry and importance of the predictors

mtry <- tuneRF(w_BCN,w_BCN$energy_demand, ntreeTry=InflectionIndex,
              stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]

rf_modified <-randomForest(energy_demand~.,data=w_BCN, mtry = best.m, importance=TRUE,ntree=InflectionIndex)



varImpPlot(randomForest(energy_demand~.,data=w_BCN, mtry = best.m, importance=TRUE,ntree=InflectionIndex), main = "Importance of the variables", n.var = 13)

best.m_BCN <- best.m

pdf("VarImportance_BCN.pdf",         # File name
    width = 14, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   

varImpPlot(randomForest(energy_demand~.,data=w_BCN, mtry = best.m, importance=TRUE,ntree=InflectionIndex), main = "Importance of the variables", n.var = 13)
# Closing the graphical device
dev.off()

```

### check performance

```{r}
# check the time
# system.time(randomForest(energy_demand~.,data=w_BCN, mtry = best.m, importance=TRUE,ntree=InflectionIndex))
```


```{r}
# assess the test set performance of rf_modified with Metrics library
predvalue <- predict(rf_modified, e_BCN)
rmse(e_BCN$energy_demand, predvalue)

pdf("BCN1.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model

plot(e_BCN$energy_demand, predvalue, xlab = "Actual energy demand for the evaluation set ", ylab = "Predicted energy demand for the evaluation set", main = "Barcelona (Tree)", cex.lab=0.75)
abline(a=0,b=1,col="blue")
grid(10,10)
dev.off()
```

```{r}
diffpct <- c((abs(e_BCN$energy_demand - predvalue))/e_BCN$energy_demand)
print(paste("The mean difference between the",nrow(e_BCN),"evaluation data set actual and model predicted values is",round(100*mean(diffpct),2),"%."))
```


### ranger modeling

```{r}
# create RF model with ranger package
ranger(formula = energy_demand ~ ., data = w_BCN)

```

### check performance

```{r}
# check execution time
# system.time(rf_ranger <- ranger(formula = energy_demand ~ ., data = w_BCN))
rf_ranger <- ranger(formula = energy_demand ~ ., data = w_BCN)
```


```{r}
# performance
rmse(
  e_BCN$energy_demand, 
  predict(rf_ranger, e_BCN)$predictions
  )

pdf("BCN2.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model

plot(e_BCN$energy_demand, 
  predict(rf_ranger, e_BCN)$predictions, ylab = "Prediction", xlab = "Energy Demand of Test Set", main = "Ranger for Barcelona")
grid(10,10)
dev.off()





```




## Code for Bilbao

### organize data


```{r}
# import data raw data set
w_BLB_raw <- read.csv("data/working_Bilbao.csv")
e_BLB_raw <- read.csv("data/evaluation_Bilbao.csv")
w_BLB <- w_BLB_raw[,-c(1,2)]
e_BLB <- e_BLB_raw[,-c(1,2)]
```

```{r}
# changes of data type
class(w_BLB$day_night) = "category"
class(w_BLB$time_band) = "category"
class(w_BLB$season) = "category"
class(w_BLB$weather_main) = "category"

class(e_BLB$day_night) = "category"
class(e_BLB$time_band) = "category"
class(e_BLB$season) = "category"
class(e_BLB$weather_main) = "category"

# check data types
str(w_BLB)
str(e_BLB) 
summary(w_BLB)

```


### RF modeling

```{r}
# fit random forest model
# default RF model
set.seed(1234)
rf2 <- randomForest(formula = energy_demand ~ ., data = w_BLB)

# find the num of trees in the forest with smallest MSE
plot(rf2)

summary(rf2)
n2 <- which.min(rf2$mse)

```





```{r}

testBLB <- rf2$mse
testBLB <- as.data.frame(testBLB)


threshold = 0.05 # Threshold Error Margin (5 %) 

#Visually, the slope become flat around 100 in the plot; determine 95% threshold value
InflectionValue <- testBLB[nrow(testBLB),] + threshold*testBLB[nrow(testBLB),]

InflectionIndex <- length(testBLB[testBLB>InflectionValue]) + 1
InflectionIndex_BLB <- InflectionIndex
print(paste("95% of the of the error has been reduced with",InflectionIndex_BLB,"trees."))
```



```{r fig.height=4}
# modify the model with best mtry and importance of the predictors

mtry <- tuneRF(w_BLB,w_BLB$energy_demand, ntreeTry=InflectionIndex,
              stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]

rf2_modified <-randomForest(energy_demand~.,data=w_BLB, mtry = best.m, importance=TRUE,ntree=InflectionIndex)


varImpPlot(randomForest(energy_demand~.,data=w_BLB, mtry = best.m, importance=TRUE,ntree=InflectionIndex), main = "Importance of the varibales", n.var = 13)

best.m_BLB <- best.m

pdf("VarImportance_BLB.pdf",         # File name
    width = 14, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   

varImpPlot(randomForest(energy_demand~.,data=w_BLB, mtry = best.m, importance=TRUE,ntree=InflectionIndex), main = "Importance of the variables", n.var = 13)
# Closing the graphical device
dev.off()


```

### check performance

```{r}
# check the time
# system.time(randomForest(formula = energy_demand ~ ., 
  #                 data = w_BLB, ntree = n2, na.action=na.exclude))
```


```{r}
# assess the test set performance of rf_modified with Metrics library
predvalue <- predict(rf2_modified, e_BLB)
rmse(e_BLB$energy_demand, predvalue)

pdf("BLB1.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model

plot(e_BLB$energy_demand, predvalue, xlab = "Energy Demand of Test Set ", ylab = "Prediction", main = "Bilbao (Tree)")
grid(10,10)
dev.off()



plot(e_BLB$energy_demand, predvalue, xlab = "Energy Demand of Test Set ", ylab = "Prediction", main = "rF for Bilbao")
```

```{r}
diffpct <- c((abs(e_BLB$energy_demand - predvalue))/e_BLB$energy_demand)
print(paste("The mean difference between the",nrow(e_BLB),"evaluation data set actual and model predicted values is",round(100*mean(diffpct),2),"%."))
```


### ranger modeling

```{r}
# create RF model with ranger package
rf2_ranger <- ranger(formula = energy_demand ~ ., data = w_BLB)

```

### check performance

```{r}
# check execution time
# system.time(rf2_ranger <- ranger(formula = energy_demand ~ ., data = w_BLB))
```


```{r}
# performance
rmse(
  e_BLB$energy_demand, 
  predict(rf2_ranger, e_BLB)$predictions
  )

pdf("BLB2.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model

plot(
  e_BLB$energy_demand, 
  predict(rf2_ranger, e_BLB)$predictions, ylab = "Prediction", xlab = "Energy Demand of Test Set", main = "Ranger for Bilbao"
  )
grid(10,10)
dev.off()


plot(
  e_BLB$energy_demand, 
  predict(rf2_ranger, e_BLB)$predictions, ylab = "Prediction", xlab = "Energy Demand of Test Set", main = "Ranger for Bilbao"
  )
```



## Code for Madrid

### organize data


```{r}
# import data raw data set
w_MDD_raw <- read.csv("data/working_Madrid.csv")
e_MDD_raw <- read.csv("data/evaluation_Madrid.csv")
w_MDD <- w_MDD_raw[,-c(1,2)]
e_MDD <- e_MDD_raw[,-c(1,2)]
```

```{r}
# changes of data type
class(w_MDD$day_night) = "category"
class(w_MDD$time_band) = "category"
class(w_MDD$season) = "category"
class(w_MDD$weather_main) = "category"

class(e_MDD$day_night) = "category"
class(e_MDD$time_band) = "category"
class(e_MDD$season) = "category"
class(e_MDD$weather_main) = "category"

# check data types
str(w_MDD)
str(e_MDD) 
summary(w_MDD)

```

### RF modeling

```{r}
# fit random forest model
# default RF model
set.seed(1234)
rf3 <- randomForest(formula = energy_demand ~ ., data = w_MDD)

# find the num of trees in the forest with smallest MSE
plot(rf3)

summary(rf3)


```



```{r}

testMDD <- rf3$mse
testMDD <- as.data.frame(testMDD)

#the slope become flat around 100 in the plot; determine 95% threshold value

threshold = 0.05 # Threshold Error Margin (5 %) 

#Visually, the slope become flat around 100 in the plot, divide test dataset at 100
InflectionValue <- testMDD[nrow(testMDD),] + threshold*testMDD[nrow(testMDD),]

InflectionIndex <- length(testMDD[testMDD>InflectionValue]) + 1
InflectionIndex_MDD <- InflectionIndex
print(paste("95% of the of the error has been reduced with",InflectionIndex_MDD,"trees."))

```



```{r fig.height=4}
# modify the model with best mtry and importance of the predictors

mtry <- tuneRF(w_MDD, w_MDD$energy_demand, ntreeTry=InflectionIndex,
              stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]

rf3_modified <-randomForest(energy_demand~.,data=w_MDD, mtry = best.m, importance=TRUE,ntree=InflectionIndex)


varImpPlot(randomForest(energy_demand~.,data=w_MDD, mtry = best.m, importance=TRUE,ntree=InflectionIndex), main = "Importance of the variables", n.var = 13)

best.m_MDD <- best.m

pdf("VarImportance_MDD.pdf",         # File name
    width = 14, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   

varImpPlot(randomForest(energy_demand~.,data=w_MDD, mtry = best.m, importance=TRUE,ntree=InflectionIndex), main = "Importance of the variables", n.var = 13)
# Closing the graphical device
dev.off()


```

### check performance

```{r}
# check the time
#system.time(randomForest(formula = energy_demand ~ ., 
  #                 data = w_MDD, ntree = 100, na.action=na.exclude))
```


```{r}
# assess the test set performance of rf_modified with Metrics library
predvalue <- predict(rf3_modified, e_MDD)
rmse(e_MDD$energy_demand, predvalue)

pdf("MDD1.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model

plot(e_MDD$energy_demand, predvalue, ylab = "Prediction", xlab = "Energy Demand of Test Set", main = "Madrid (Tree)")

grid(10,10)
dev.off()

plot(e_MDD$energy_demand, predvalue, ylab = "Prediction", xlab = "Energy Demand of Test Set", main = "rF for Madrid")

```

```{r}
diffpct <- c((abs(e_MDD$energy_demand - predvalue))/e_MDD$energy_demand)
print(paste("The mean difference between the",nrow(e_MDD),"evaluation data set actual and model predicted values is",round(100*mean(diffpct),2),"%."))
```


### ranger modeling

```{r}
# create RF model with ranger package
rf3_ranger <- ranger(formula = energy_demand ~ ., data = w_MDD)

```

### check performance

```{r}
# check execution time
# system.time(rf3_ranger <- ranger(formula = energy_demand ~ ., data = w_MDD))
```


```{r}
# performance
rmse(
  e_MDD$energy_demand, 
  predict(rf3_ranger, e_MDD)$predictions
  )

pdf("MDD2.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model

plot(
  e_MDD$energy_demand, 
  predict(rf3_ranger, e_MDD)$predictions, ylab = "Prediction", xlab = "Energy Demand of Test Set", main = "Ranger for Madrid"
  )
grid(10,10)
dev.off()

plot(
  e_MDD$energy_demand, 
  predict(rf3_ranger, e_MDD)$predictions, ylab = "Prediction", xlab = "Energy Demand of Test Set", main = "Ranger for Madrid"
  )
```



## Code for Seville

### organize data


```{r}
# import data raw data set
w_SVL_raw <- read.csv("data/working_Seville.csv")
e_SVL_raw <- read.csv("data/evaluation_Seville.csv")
w_SVL <- w_SVL_raw[,-c(1,2)]
e_SVL <- e_SVL_raw[,-c(1,2)]
```

```{r}
# changes of data type
class(w_SVL$day_night) = "category"
class(w_SVL$time_band) = "category"
class(w_SVL$season) = "category"
class(w_SVL$weather_main) = "category"

class(e_SVL$day_night) = "category"
class(e_SVL$time_band) = "category"
class(e_SVL$season) = "category"
class(e_SVL$weather_main) = "category"

# check data types
str(w_SVL)
str(e_SVL) 
summary(w_SVL)

```

### RF modeling

```{r}
# fit random forest model
# default RF model
set.seed(1234)
rf4 <- randomForest(formula = energy_demand ~ ., data = w_SVL)
# find the num of trees in the forest with smallest MSE
plot(rf4)

```

```{r}

testSVL <- rf4$mse
testSVL <- as.data.frame(testSVL)

#the slope become flat around 100 in the plot; determine 95% threshold value

threshold = 0.05 # Threshold Error Margin (5 %) 

#Visually, the slope become flat around 100 in the plot, divide test dataset at 100
InflectionValue <- testSVL[nrow(testSVL),] + threshold*testSVL[nrow(testSVL),]

InflectionIndex <- length(testSVL[testSVL>InflectionValue]) + 1
InflectionIndex_SVL <- InflectionIndex
print(paste("95% of the of the error has been reduced with",InflectionIndex_SVL,"trees."))

```



```{r fig.height=4}
# modify the model with best mtry and importance of the predictors

mtry <- tuneRF(w_SVL, w_SVL$energy_demand, ntreeTry=InflectionIndex,
              stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]

rf4_modified <-randomForest(energy_demand~.,data=w_SVL, mtry = best.m, importance=TRUE,ntree=InflectionIndex)


varImpPlot(randomForest(energy_demand~.,data=w_SVL, mtry = best.m, importance=TRUE,ntree=InflectionIndex), main = "Importance of the variables", n.var = 13)

best.m_SVL <- best.m

pdf("VarImportance_SVL.pdf",         # File name
    width = 14, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   

varImpPlot(randomForest(energy_demand~.,data=w_SVL, mtry = best.m, importance=TRUE,ntree=InflectionIndex), main = "Importance of the variables", n.var = 13)
# Closing the graphical device
dev.off()

```





### check performance

```{r}
# check the time
# system.time(randomForest(formula = energy_demand ~ ., 
#                 data = w_SVL, ntree = n4, na.action=na.exclude))
```


```{r}
# assess the test set performance of rf_modified with Metrics library
predvalue <- predict(rf4_modified, e_SVL)
rmse(e_SVL$energy_demand, predvalue)
plot(e_SVL$energy_demand, predvalue, main = "Seville1")

pdf("SVL1.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model

plot(e_SVL$energy_demand, predvalue, ylab = "Prediction", xlab = "Energy Demand of Test Set", main = "Seville (Tree)")

grid(10,10)
dev.off()

```


```{r}
diffpct <- c((abs(e_SVL$energy_demand - predvalue))/e_SVL$energy_demand)
print(paste("The mean difference between the",nrow(e_SVL),"evaluation data set actual and model predicted values is",round(100*mean(diffpct),2),"%."))
```

### ranger modeling

```{r}
# create RF model with ranger package
rf4_ranger <- ranger(formula = energy_demand ~ ., data = w_SVL)

```

### check performance

```{r}
# check execution time
# system.time(rf4_ranger <- ranger(formula = energy_demand ~ ., data = w_SVL))
```





```{r}
# performance
rmse(
  e_SVL$energy_demand, 
  predict(rf4_ranger, e_SVL)$predictions
  )
plot(
  e_SVL$energy_demand, 
  predict(rf4_ranger, e_SVL)$predictions, xlab = "predvalue", main = "Seville2"
  )

pdf("SVL2.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model

plot(
  e_SVL$energy_demand, 
  predict(rf4_ranger, e_SVL)$predictions, ylab = "Prediction", xlab = "Energy Demand of Test Set", main = "Ranger for Seville"
  )
grid(10,10)
dev.off()
```


## Code for Valencia

### organize data


```{r}
# import data raw data set
w_VLC_raw <- read.csv("data/working_Valencia.csv")
e_VLC_raw <- read.csv("data/evaluation_Valencia.csv")
w_VLC <- w_VLC_raw[,-c(1,2)]
e_VLC <- e_VLC_raw[,-c(1,2)]
```

```{r}
# changes of data type
class(w_VLC$day_night) = "category"
class(w_VLC$time_band) = "category"
class(w_VLC$season) = "category"
class(w_VLC$weather_main) = "category"

class(e_VLC$day_night) = "category"
class(e_VLC$time_band) = "category"
class(e_VLC$season) = "category"
class(e_VLC$weather_main) = "category"

# check data types
str(w_VLC)
str(e_VLC) 
summary(w_VLC)

```

### RF modeling

```{r}
# fit random forest model
# default RF model
set.seed(1234)
rf5 <- randomForest(formula = energy_demand ~ ., data = w_VLC)
# find the num of trees in the forest with smallest MSE
plot(rf5)

```

```{r}

testVLC <- rf5$mse
testVLC <- as.data.frame(testVLC)

#the slope become flat around 100 in the plot; determine 95% threshold value

threshold = 0.05 # Threshold Error Margin (5 %) 

#Visually, the slope become flat around 100 in the plot, divide test dataset at 100
InflectionValue <- testVLC[nrow(testVLC),] + threshold*testVLC[nrow(testVLC),]

InflectionIndex <- length(testVLC[testVLC>InflectionValue]) + 1
InflectionIndex_VLC <- InflectionIndex
print(paste("95% of the of the error has been reduced with",InflectionIndex_VLC,"trees."))

```



```{r fig.height=4}
# modify the model with best mtry and importance of the predictors

mtry <- tuneRF(w_VLC, w_VLC$energy_demand, ntreeTry=InflectionIndex,
              stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]

rf5_modified <-randomForest(energy_demand~.,data=w_VLC, mtry = best.m, importance=TRUE,ntree=InflectionIndex)


varImpPlot(randomForest(energy_demand~.,data=w_VLC, mtry = best.m, importance=TRUE,ntree=InflectionIndex), main = "Importance of the variables", n.var = 13)

best.m_VLC <- best.m

pdf("VarImportance_VLC.pdf",         # File name
    width = 14, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model
   

varImpPlot(randomForest(energy_demand~.,data=w_VLC, mtry = best.m, importance=TRUE,ntree=InflectionIndex), main = "Importance of the variables", n.var = 13)
# Closing the graphical device
dev.off()

```




### check performance

```{r}
# check the time
# system.time(randomForest(formula = energy_demand ~ ., 
   #                 data = w_VLC, ntree = n5, na.action=na.exclude))
```


```{r}
# assess the test set performance of rf_modified with Metrics library
predvalue <- predict(rf5_modified, e_VLC)
rmse(e_VLC$energy_demand, predvalue)
plot(e_VLC$energy_demand, predvalue, main = "Valencia1")

pdf("VLC1.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model

plot(e_VLC$energy_demand, predvalue, ylab = "Prediction", xlab = "Energy Demand of Test Set", main = "Valencia  (Tree)")

grid(10,10)
dev.off()
```

```{r}
diffpct <- c((abs(e_VLC$energy_demand - predvalue))/e_VLC$energy_demand)
print(paste("The mean difference between the",nrow(e_VLC),"evaluation data set actual and model predicted values is",round(100*mean(diffpct),2),"%."))
```


### ranger modeling

```{r}
# create RF model with ranger package
rf5_ranger <- ranger(formula = energy_demand ~ ., data = w_VLC)

```

### check performance

```{r}
# check execution time
# system.time(rf5_ranger <- ranger(formula = energy_demand ~ ., data = w_VLC))
```


```{r}
# performance
rmse(
  e_VLC$energy_demand, 
  predict(rf5_ranger, e_VLC)$predictions
  )
plot(
  e_VLC$energy_demand, 
  predict(rf5_ranger, e_VLC)$predictions, xlab = "predvalue", main = "Valencia2"
  )

pdf("VLC2.pdf",         # File name
    width = 7, height = 4, # Width and height in inches
    bg = "white",          # Background color
    colormodel = "cmyk")  # Color model

plot(
  e_VLC$energy_demand, 
  predict(rf5_ranger, e_VLC)$predictions, ylab = "Prediction", xlab = "Energy Demand of Test Set", main = "Ranger for Valencia"
  )
grid(10,10)
dev.off()
```







